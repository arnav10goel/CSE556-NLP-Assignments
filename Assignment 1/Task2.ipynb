{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:89: UserWarning: `return_all_scores` is now deprecated, use `top_k=1` if you want similar functionnality\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from utils import emotion_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Bigram Language Model Class\n",
    "class BigramLM:\n",
    "    def __init__(self):\n",
    "        self.bigram_freq = {}\n",
    "        self.total_freq = {}\n",
    "        self.vocab = set()\n",
    "        self.vocab_size = 0\n",
    "        self.corpus_size = 0\n",
    "        self.corpus = [] \n",
    "        self.labels = []\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        # Preprocess the text\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove all non-alphanumeric characters\n",
    "        text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "        return text\n",
    "    \n",
    "    def train_lm(self, corpus, labels):\n",
    "        # Open the corpus and labels file and read it\n",
    "        with open(corpus, 'r', encoding='utf-8') as corpus_file, open(labels, 'r', encoding='utf-8') as labels_file:\n",
    "            for line in corpus_file:\n",
    "                tokens = self.preprocess_text(line).split()\n",
    "                self.corpus.append(tokens)\n",
    "\n",
    "            for line in labels_file:\n",
    "                self.labels.append(line.strip())\n",
    "        \n",
    "        # Get the vocabulary\n",
    "        self.vocab = set([word for sentence in self.corpus for word in sentence])\n",
    "\n",
    "        # Get the vocabulary size\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "        # Get the corpus size\n",
    "        self.corpus_size = len(self.corpus)\n",
    "\n",
    "        for sentence in self.corpus:\n",
    "            for i in range(len(sentence) - 1):\n",
    "\n",
    "                # Get the bigram frequencies\n",
    "                bigram = (sentence[i], sentence[i + 1])\n",
    "                self.bigram_freq[bigram] = self.bigram_freq.get(bigram, 0) + 1\n",
    "\n",
    "                # Get the unigram frequencies\n",
    "                unigram = sentence[i]\n",
    "                self.total_freq[unigram] = self.total_freq.get(unigram, 0) + 1\n",
    "                if(i == len(sentence) - 2):\n",
    "                    unigram = sentence[i + 1]\n",
    "                    self.total_freq[unigram] = self.total_freq.get(unigram, 0) + 1\n",
    "\n",
    "    # Get the probability of a bigram without smoothing\n",
    "    def get_bigram_prob(self, previous_word, next_word):\n",
    "        bigram = (previous_word, next_word)\n",
    "        # If the bigram is not present in the corpus, return 0\n",
    "        if bigram not in self.bigram_freq:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.bigram_freq[bigram] / self.total_freq[previous_word]\n",
    "        \n",
    "    # Q2 \n",
    "    # Get the probability of a bigram with laplace smoothing\n",
    "    def get_bigram_prob_laplace(self, previous_word, next_word):\n",
    "        bigram = (previous_word, next_word)\n",
    "        \n",
    "        # Retrieve frequency of bigram and add 1\n",
    "        bigram_freq_smoothed = self.bigram_freq.get(bigram, 0) + 1\n",
    "\n",
    "        # Retrieve frequency of previous word and add the vocabulary size\n",
    "        previous_word_freq_smoothed = self.total_freq.get(previous_word, 0) + self.vocab_size\n",
    "\n",
    "        # Return the probability\n",
    "        return bigram_freq_smoothed / previous_word_freq_smoothed\n",
    "    \n",
    "    # Get the probability of a bigram with kneser-ney smoothing\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_lm = BigramLM()\n",
    "\n",
    "bigram_lm.train_lm('data/corpus.txt', 'data/labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of bigram without smoothing:  0\n",
      "Probability of bigram with laplace smoothing:  0.0001841959845275373\n"
     ]
    }
   ],
   "source": [
    "# Get the probability of a bigram without smoothing\n",
    "print('Probability of bigram without smoothing: ', bigram_lm.get_bigram_prob('arnav', 'medha')) \n",
    "\n",
    "# Get the probability of a bigram with laplace smoothing\n",
    "print('Probability of bigram with laplace smoothing: ', bigram_lm.get_bigram_prob_laplace('arnav', 'medha'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
