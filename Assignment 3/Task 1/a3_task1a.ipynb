{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4twyoAcQZhC1",
        "outputId": "c7d873c1-5b9f-4aff-f115-a8b7d70120be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.99)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv('/content/train.csv', sep='\\t')\n",
        "dev_df = pd.read_csv('/content/dev.csv', sep='\\t')\n",
        "\n",
        "dev_df.rename(columns={'setence1': 'sentence1'}, inplace=True)"
      ],
      "metadata": {
        "id": "In9ZDlKfaWXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNl-cosqvMuj",
        "outputId": "230e89f3-83ff-49c6-aef0-d41b603166b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Punctuation removal\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized_text = ' '.join([lemmatizer.lemmatize(token) for token in tokens])\n",
        "\n",
        "    return lemmatized_text"
      ],
      "metadata": {
        "id": "M9t-ZFwdw84K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Task1A_Dataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.dataframe = dataframe\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text1 = str(self.dataframe.iloc[idx]['sentence1'])\n",
        "        text2 = str(self.dataframe.iloc[idx]['sentence2'])\n",
        "\n",
        "        sentence1 = preprocess_text(text1)\n",
        "        sentence2 = preprocess_text(text2)\n",
        "\n",
        "        score = self.dataframe.iloc[idx]['score']\n",
        "\n",
        "        # Tokenize the pair of sentences to get the token ids, attention masks, and token type ids\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            sentence1, sentence2,\n",
        "            add_special_tokens=True, #cls and sep\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
        "            'labels': torch.tensor(score, dtype=torch.float)\n",
        "        }"
      ],
      "metadata": {
        "id": "eNrHiBZDbQRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'df' is your DataFrame\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Initialize the dataset\n",
        "train_dataset = Task1A_Dataset(train_df, tokenizer)\n",
        "dev_dataset = Task1A_Dataset(dev_df, tokenizer)\n",
        "\n",
        "# Create a DataLoader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=1024, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEW764z2cRo6",
        "outputId": "ada11808-9cd7-4570-bdd6-afc60d4625fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BertForTextSimilarity(nn.Module):\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        super(BertForTextSimilarity, self).__init__()\n",
        "        # Load pre-trained BERT model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Add a linear layer for regression\n",
        "        self.regression = nn.Linear(self.bert.config.hidden_size, 1)\n",
        "\n",
        "        # Option to freeze BERT layers to prevent them from being updated during training\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Get the output from BERT model\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # The first token of every sequence is a special token ([CLS]) that contains the aggregate representation for classification tasks. We use it for regression here.\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
        "\n",
        "        # Pass the [CLS] token's output through the regression layer\n",
        "        score = self.regression(cls_output)  # Shape: (batch_size, 1)\n",
        "\n",
        "        return score\n"
      ],
      "metadata": {
        "id": "oczkM81Tyzl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForTextSimilarity()\n",
        "\n",
        "# Check if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using CUDA:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CUDA is not available. Using CPU instead.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2qzdAq8y0W_",
        "outputId": "3ddc93c1-9967-4364-8030-afa0e18281e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is not available. Using CPU instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "mse_loss = torch.nn.MSELoss()\n",
        "\n",
        "num_epochs = 3  # or however many epochs you plan to train for\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    train_loss = 0\n",
        "    i = 0\n",
        "    for batch in train_dataloader:\n",
        "        print(i)\n",
        "        i+=1\n",
        "        # Forward pass\n",
        "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
        "        loss = mse_loss(outputs.squeeze(), batch['labels'])\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # After each epoch, do validation\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():  # No need to compute gradients during validation\n",
        "        for batch in dev_dataloader:\n",
        "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
        "            loss = mse_loss(outputs.squeeze(), batch['labels'])\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # Calculate average losses\n",
        "    avg_train_loss = train_loss / len(train_dataloader)\n",
        "    avg_val_loss = val_loss / len(dev_dataloader)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG0vHWnDvqdb",
        "outputId": "0a995c94-0296-48c8-efbe-1e2051b4986b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    }
  ]
}