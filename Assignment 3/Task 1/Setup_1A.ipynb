{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "In9ZDlKfaWXT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv('A3_task1_data_files/train.csv', sep='\\t')\n",
        "dev_df = pd.read_csv('A3_task1_data_files/dev.csv', sep='\\t')\n",
        "\n",
        "dev_df.rename(columns={'setence1': 'sentence1'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNl-cosqvMuj",
        "outputId": "230e89f3-83ff-49c6-aef0-d41b603166b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/arnav/miniconda3/envs/DLA2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[nltk_data] Downloading package punkt to /home/arnav/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/arnav/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /home/arnav/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/arnav/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "M9t-ZFwdw84K"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Punctuation removal\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized_text = ' '.join([lemmatizer.lemmatize(token) for token in tokens])\n",
        "\n",
        "    return lemmatized_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eNrHiBZDbQRt"
      },
      "outputs": [],
      "source": [
        "class Task1A_Dataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.dataframe = dataframe\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text1 = str(self.dataframe.iloc[idx]['sentence1'])\n",
        "        text2 = str(self.dataframe.iloc[idx]['sentence2'])\n",
        "\n",
        "        sentence1 = preprocess_text(text1)\n",
        "        sentence2 = preprocess_text(text2)\n",
        "\n",
        "        score = self.dataframe.iloc[idx]['score']\n",
        "\n",
        "        # Tokenize the pair of sentences to get the token ids, attention masks, and token type ids\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            sentence1, sentence2,\n",
        "            add_special_tokens=True, #cls and sep\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
        "            'labels': torch.tensor(score, dtype=torch.float)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEW764z2cRo6",
        "outputId": "ada11808-9cd7-4570-bdd6-afc60d4625fb"
      },
      "outputs": [],
      "source": [
        "# Assuming 'df' is your DataFrame\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Initialize the dataset\n",
        "train_dataset = Task1A_Dataset(train_df, tokenizer)\n",
        "dev_dataset = Task1A_Dataset(dev_df, tokenizer)\n",
        "\n",
        "# Create a DataLoader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oczkM81Tyzl6"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BertForTextSimilarity(nn.Module):\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        super(BertForTextSimilarity, self).__init__()\n",
        "        # Load pre-trained BERT model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Add a linear layer for regression\n",
        "        self.regression = nn.Linear(self.bert.config.hidden_size, 1)\n",
        "\n",
        "        # Option to freeze BERT layers to prevent them from being updated during training\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Get the output from BERT model\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # The first token of every sequence is a special token ([CLS]) that contains the aggregate representation for classification tasks. We use it for regression here.\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
        "\n",
        "        # Pass the [CLS] token's output through the regression layer\n",
        "        score = self.regression(cls_output)  # Shape: (batch_size, 1)\n",
        "\n",
        "        return score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2qzdAq8y0W_",
        "outputId": "3ddc93c1-9967-4364-8030-afa0e18281e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CUDA: NVIDIA RTX A5000\n"
          ]
        }
      ],
      "source": [
        "model = BertForTextSimilarity()\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    model.to(device)  # Move model to CUDA device if available\n",
        "    print(\"Using CUDA:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CUDA is not available. Using CPU instead.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DG0vHWnDvqdb",
        "outputId": "0a995c94-0296-48c8-efbe-1e2051b4986b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "86\n",
            "87\n",
            "88\n",
            "89\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3, Train Loss: 1.6999, Validation Loss: 0.8666\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "43\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "44\n",
            "45\n",
            "46\n",
            "47\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/3, Train Loss: 0.6276, Validation Loss: 0.7618\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32\n",
            "33\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.4429, Validation Loss: 0.7224\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "mse_loss = torch.nn.MSELoss()\n",
        "\n",
        "num_epochs = 3  # or however many epochs you plan to train for\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    train_loss = 0\n",
        "    i = 0\n",
        "    for batch in train_dataloader:\n",
        "        print(i)\n",
        "        i+=1\n",
        "        # Forward pass\n",
        "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "        loss = mse_loss(outputs.squeeze(), inputs['labels'])\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # After each epoch, do validation\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():  # No need to compute gradients during validation\n",
        "        for batch in dev_dataloader:\n",
        "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "            loss = mse_loss(outputs.squeeze(), inputs['labels'])\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    # Calculate average losses\n",
        "    avg_train_loss = train_loss / len(train_dataloader)\n",
        "    avg_val_loss = val_loss / len(dev_dataloader)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pearson Correlation: 0.8401, P-Value: 0.0000\n"
          ]
        }
      ],
      "source": [
        "# Calculate similarity scores on the validation set\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "predictions = []\n",
        "labels = []\n",
        "\n",
        "with torch.no_grad():  # No need to compute gradients during validation\n",
        "    for batch in dev_dataloader:\n",
        "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "        predictions.extend(outputs.squeeze().tolist())\n",
        "        labels.extend(inputs['labels'].tolist())\n",
        "\n",
        "# Calculate Pearson correlation\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "correlation, p_value = pearsonr(labels, predictions)\n",
        "\n",
        "print(f'Pearson Correlation: {correlation:.4f}, P-Value: {p_value:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4.693336009979248, 4.7085442543029785, 4.358657360076904, 3.1744496822357178, 3.671196460723877, 3.614952802658081, 4.799011707305908, 2.4625680446624756, 4.583574295043945, 4.747319221496582, 3.5506203174591064, 2.159536838531494, 4.74253511428833, 4.858457088470459, 4.544525623321533, 0.8718488812446594, 3.7103030681610107, 4.8556132316589355, 4.238290309906006, 1.1416125297546387, 3.2388880252838135, 1.878218412399292, 3.5133578777313232, 1.922269344329834, 0.9625773429870605, 3.0363857746124268, 3.8730363845825195, 3.7058303356170654, 0.883797287940979, 0.2812442481517792, 4.1213483810424805, 4.435329914093018, 4.539696216583252, 4.50732421875, 4.3185625076293945, 4.561171054840088, 4.5720295906066895, 4.581595420837402, 4.069953441619873, 1.1741275787353516, 3.9322822093963623, 4.195449352264404, 2.73103666305542, 4.084671974182129, 1.4325447082519531, 0.43928900361061096, 0.04780564829707146, 0.18695789575576782, 2.929619073867798, 4.169259071350098, 0.5952378511428833, 4.785333156585693, 4.265263557434082, 4.531497478485107, 0.41571375727653503, 2.5228686332702637, 0.38411322236061096, 0.36454421281814575, 2.740560531616211, -0.04234464094042778, 2.8079702854156494, 3.7509748935699463, 2.962681293487549, 4.334836483001709, 3.804072618484497, 0.18058764934539795, 4.234649181365967, 4.805173397064209, 2.938797950744629, 3.5438404083251953, 3.7373809814453125, 2.4254791736602783, 3.3761560916900635, 0.24125918745994568, 0.3782244324684143, 4.631953716278076, 2.888458013534546, 2.979266881942749, 1.5823911428451538, 4.5245041847229, 3.148855447769165, -0.08895987272262573, 0.08020973205566406, 3.330460786819458, 4.838599681854248, 0.4120694100856781, 1.4925541877746582, 4.196196556091309, 4.313802242279053, 4.199597358703613, 3.301560163497925, 1.5054682493209839, 1.9482262134552002, 0.24575617909431458, 0.5161048769950867, 3.2283852100372314, 0.3978358507156372, 0.6137173175811768, 0.06747549772262573, 3.0229294300079346, -0.03466613218188286, 0.44031259417533875, 3.4961330890655518, 2.74457049369812, 0.21199658513069153, 4.059455394744873, 4.364025115966797, 3.6822128295898438, 0.8936047554016113, 1.4689401388168335, 1.054124116897583, 0.13923054933547974, 0.15415170788764954, 0.37607282400131226, -0.0041918642818927765, 0.16302844882011414, 3.0833351612091064, 3.05983829498291, 0.04419150575995445, 0.2155228555202484, 2.765930414199829, 0.0107954703271389, 2.9229962825775146, 0.9689026474952698, 1.552281379699707, 2.5324437618255615, 0.03607810288667679, 0.11598318815231323, 1.879892110824585, 3.5035884380340576, 0.10666429996490479, 4.190816402435303, 4.72829008102417, 4.630856037139893, 4.562160015106201, 4.562160015106201, 4.355072021484375, 3.1266491413116455, 2.3371007442474365, 3.598116397857666, 1.0551693439483643, 4.750437259674072, 4.808290004730225, 2.5467379093170166, 4.564156532287598, 0.7989740371704102, 0.8606038093566895, 3.9919846057891846, 1.922269344329834, 1.5811768770217896, 3.935593366622925, 3.4214961528778076, 4.536533355712891, 2.0761990547180176, 0.8482562303543091, 2.4439504146575928, 0.6587226390838623, 0.6441970467567444, 4.269186973571777, 1.580049753189087, 1.9210124015808105, 0.845314085483551, 5.022879123687744, 3.4528133869171143, 0.6853246688842773, 2.9651920795440674, 4.208146095275879, 2.4325110912323, 0.6276616454124451, 3.6545209884643555, 3.9126856327056885, 4.367827892303467, 4.3981032371521, 0.12044577300548553, 3.9482054710388184, 4.616774082183838, 0.9064158201217651, 3.6419734954833984, 1.6124876737594604, 1.9877750873565674, 0.9108651280403137, 1.3214905261993408, 1.1430467367172241, 0.8214880228042603, 3.9962961673736572, 4.436272144317627, 0.5096356868743896, 4.070583820343018, 0.3673635423183441, 4.264328956604004, 4.59274435043335, 1.7617849111557007, 3.286486864089966, 3.6827011108398438, 2.4230129718780518, 0.7248586416244507, 0.3182337284088135, 3.7460744380950928, 4.5278143882751465, 0.03294416889548302, 0.07367299497127533, 0.21203605830669403, 0.04679800197482109, 1.8989965915679932, 0.731095016002655, 0.7912358045578003, 0.20670944452285767, 2.3506290912628174, 2.841956377029419, 0.7027734518051147, 0.44969484210014343, 1.0593669414520264, 2.203627824783325, 0.7661275863647461, 1.0632643699645996, 0.5361629128456116, 1.948455572128296, 0.28559789061546326, 3.1236989498138428, 0.8273830413818359, 4.327159404754639, 0.31792953610420227, 2.629563570022583, 1.303181529045105, 3.6472959518432617, 4.079778671264648, 0.26648131012916565, 4.065418243408203, 0.711219310760498, 0.18404483795166016, 2.113114595413208, 0.18090084195137024, 0.46988677978515625, 2.683408260345459, 3.745443105697632, 3.207763433456421, 2.301226854324341, 3.664220094680786, 4.6620402336120605, 0.24672380089759827, 0.5785004496574402, 2.6945250034332275, 0.07262445986270905, 1.1012346744537354, 0.48578551411628723, 0.1732211410999298, 0.029763277620077133, 3.6915509700775146, 3.2555789947509766, -0.06378419697284698, 3.722111463546753, 0.08883997797966003, 1.070495843887329, 2.77823805809021, 2.6822102069854736, 2.2475531101226807, 0.28620630502700806, 3.2416586875915527, 3.4806883335113525, 2.970221996307373, 0.39097800850868225, 0.14316436648368835, 4.24006986618042, 1.260372281074524, 4.013812065124512, 4.358459949493408, 2.552380084991455, 4.407677173614502, 2.8011422157287598, 4.325465679168701, 3.4805314540863037, 2.0900418758392334, 4.041707515716553, 3.6844124794006348, 3.0909743309020996, 2.992947816848755, 0.38044804334640503, 3.781231164932251, 4.17756986618042, 0.6930403709411621, 3.127002716064453, 4.529249668121338, 4.454918384552002, 4.231781482696533, 2.061027765274048, 2.9801199436187744, 2.7699248790740967, 0.6007399559020996, -0.10185113549232483, 3.741460084915161, 4.371037483215332, 0.5092461109161377, 0.8781649470329285, 1.2235664129257202, 4.129107475280762, 3.596787929534912, 2.765087127685547, 4.50838565826416, 0.02089609205722809, 3.9300920963287354, 3.7167460918426514, 0.4488253593444824, 4.1451897621154785, 3.7522482872009277, 4.03018856048584, 1.5672228336334229, 4.432846546173096, 4.3638529777526855, 0.01700584962964058, 2.9742753505706787, 1.5321135520935059, 3.2231736183166504, 1.2066459655761719, 2.2282419204711914, 2.190537452697754, 0.1474955976009369, 3.960062265396118, 3.514275550842285, 3.7982749938964844, 0.2002420276403427, 3.746450662612915, 3.2810704708099365, 1.8976051807403564, 0.9440782070159912, 4.131473064422607, 1.4099068641662598, 4.157869815826416, 4.237463474273682, 2.8609020709991455, 2.907562255859375, 3.883105993270874, 2.4061601161956787, 3.560683012008667, 0.837116003036499, 0.3728577196598053, 4.1680908203125, 0.1742735356092453, 1.9113030433654785, 4.234905242919922, 3.1715190410614014, 0.4378509223461151, 4.123045444488525, 2.9214279651641846, 3.431889772415161, 3.54573130607605, 1.9643032550811768, 2.861149311065674, 2.499027967453003, 2.1806323528289795, 3.5917763710021973, 3.2326784133911133, 4.310683727264404, 3.8842108249664307, 3.854337692260742, 4.15067720413208, 4.0863938331604, 3.7957117557525635, 3.9448869228363037, 4.361344337463379, 0.28939172625541687, 0.1414596438407898, 4.450755596160889, 2.655913829803467, 0.39223307371139526, 4.1668853759765625, 4.3994317054748535, 0.16499869525432587, 4.011449337005615, 4.2332282066345215, 4.831672191619873, 0.18659880757331848, 1.465671420097351, 3.4594593048095703, 3.0962750911712646, 4.654726982116699, 4.64613676071167, 3.8150107860565186, 4.444852352142334, 4.518481731414795, 0.07734493911266327, 0.3160092532634735, 2.80679988861084, 4.498442649841309, 0.3451784551143646, 3.0407025814056396, 4.622344017028809, 2.7247965335845947, 4.240149021148682, 2.078791856765747, 4.485654354095459, 4.31817626953125, 4.772512912750244, 0.265993595123291, 3.326620101928711, 3.8968119621276855, 3.740600347518921, 3.9784977436065674, 4.631584644317627, 4.295300483703613, 3.3262736797332764, 3.637847900390625, 2.3102221488952637, 3.7099671363830566, 3.718806028366089, 1.2207586765289307, 3.825878143310547, 0.07067039608955383, 0.2699948251247406, 0.8169293403625488, 4.358205795288086, 3.905102491378784, 1.3812198638916016, 0.5244621634483337, 2.1618216037750244, 1.4379260540008545, 4.645308017730713, 3.9958126544952393, 3.8207485675811768, 1.3488705158233643, 2.955894947052002, 2.9741642475128174, 2.637681245803833, 0.06844198703765869, 4.661571025848389, 4.165422439575195, 4.512372970581055, 2.4114937782287598, 4.275532245635986, 0.8202964663505554, 4.057013034820557, 2.8508756160736084, 4.28025484085083, 0.04915672913193703, 4.573879718780518, 4.469118595123291, 1.8366196155548096, 2.680514097213745, 0.00951765850186348, 0.5669025182723999, 4.637786388397217, 3.827219247817993, 2.9519588947296143, 2.220555067062378, 1.5240085124969482, 3.9282233715057373, 4.526314735412598, 0.7747750282287598, 1.730876088142395, 3.721370220184326, 3.1146323680877686, 3.553352117538452, 1.9869606494903564, 4.3139729499816895, 1.4972753524780273, 1.3686988353729248, 3.3400447368621826, 0.8256161212921143, -0.11403872072696686, 4.380204677581787, 0.8682931661605835, 4.679314136505127, 3.680009126663208, 0.7452340126037598, 1.8337976932525635, 1.1317296028137207, 0.41952046751976013, 1.4672346115112305, 1.53248929977417, 1.6265885829925537, 3.37514591217041, 4.219995498657227, 3.5186078548431396, 4.432770252227783, 0.7863721251487732, 3.5759246349334717, 4.337247371673584, 4.401783466339111, 1.3799762725830078, 2.0521934032440186, 4.2867560386657715, 4.3277082443237305, 1.556246042251587, 2.8652384281158447, 0.3081530034542084, 4.72862434387207, 1.0351248979568481, 0.9048371315002441, 0.1809074580669403, 2.1856818199157715, 1.6511801481246948, 3.791046380996704, 0.22288011014461517, 3.234978199005127, 1.7415337562561035, 2.6322128772735596, 0.33409228920936584, 3.0175697803497314, 2.858074426651001, 4.225616455078125, 4.398928165435791, 1.0977799892425537, 2.4826793670654297, 2.1123757362365723, 0.404913067817688, 3.932115316390991, 2.1661698818206787, 3.512153387069702, 4.514830112457275, 0.8605980277061462, 4.305296897888184, 0.6499870419502258, 0.13320709764957428, 4.186979293823242, 4.0425238609313965, 0.7707059979438782, 4.420145511627197, 0.907929539680481, 3.4761946201324463, 4.523408889770508, 4.065207004547119, 3.5014760494232178, 1.0040678977966309, 3.523175001144409, 1.264897108078003, 4.578073978424072, 4.74253511428833, 2.0748088359832764, 2.32621169090271, 1.8049893379211426, 2.8285186290740967, 4.138153553009033, 1.3131933212280273, 4.332004547119141, 1.347097635269165, 3.3909990787506104, 3.081423044204712, 3.502629041671753, 4.837632179260254, 1.1994067430496216, 4.766765594482422, 0.6696435213088989, 0.2677115499973297, 4.219780445098877, 4.490965843200684, 0.31004849076271057, 1.4938642978668213, 3.690051555633545, 3.2933809757232666, 2.9618422985076904, -0.03251224756240845, 3.3779876232147217, 2.3122355937957764, 0.39847105741500854, 2.064333915710449, 2.0655572414398193, 1.1211607456207275, 4.827636241912842, 1.8633893728256226, 1.4190716743469238, 4.1461405754089355, 4.222519397735596, 1.8828482627868652, 3.8272688388824463, 4.585595607757568, 0.9982686042785645, 1.4586842060089111, 0.47749313712120056, 1.4406547546386719, 1.4992833137512207, 0.06963399052619934, 0.2127329558134079, 2.682187795639038, 2.4115612506866455, 0.6918736696243286, 4.461611270904541, 1.8380143642425537, 4.195407390594482, 4.408902168273926, 0.3207116425037384, 3.040492534637451, 1.3680076599121094, 3.5329859256744385, 0.7725315093994141, 4.569383144378662, 4.732927322387695, 2.02475905418396, 4.286057949066162, 2.3426809310913086, 4.770165920257568, 1.1991729736328125, 0.7452678680419922, 0.45056262612342834, 0.44688621163368225, 1.8353153467178345, 1.5536715984344482, 4.010935306549072, 4.981142520904541, 3.0874440670013428, 0.5876970887184143, 1.053083896636963, 0.7108796834945679, 4.698084831237793, 2.5815718173980713, 0.23140564560890198, 3.599071741104126, 4.277646541595459, 0.6612753868103027, 1.4511250257492065, 2.078035354614258, 4.546203136444092, 2.0465080738067627, 4.35216760635376, 1.8605222702026367, 4.403216361999512, 0.9414955377578735, 2.8328559398651123, 2.4674432277679443, 1.7614836692810059, 3.1611063480377197, 3.718776226043701, 2.9073891639709473, 0.3551352918148041, 2.9962353706359863, 4.2842488288879395, 3.7385900020599365, 3.3542912006378174, 1.1692590713500977, 2.2412068843841553, 1.6989222764968872, 1.3308125734329224, 2.1241748332977295, 2.1241910457611084, 2.9820845127105713, 2.9911649227142334, 2.0563526153564453, 1.47603178024292, 2.058380365371704, 1.5896360874176025, 1.5305819511413574, 1.6743519306182861, 1.8919557332992554, 2.324671983718872, 2.3385653495788574, 3.185697317123413, 2.1749935150146484, 2.972815752029419, 3.2667031288146973, 3.152209758758545, 2.779073476791382, 2.0660793781280518, 1.2232677936553955, 2.23382306098938, 2.721727132797241, 1.7436341047286987, 3.3043150901794434, 3.4025473594665527, 1.9010677337646484, 2.3249480724334717, 1.5492057800292969, 2.2707011699676514, 2.226989269256592, 2.90228533744812, 2.37954044342041, 2.680264711380005, 2.3867743015289307, 2.236262083053589, 2.6186344623565674, 2.059547185897827, 1.724265456199646, 2.5849666595458984, 0.6745917201042175, 3.000638246536255, 1.453942894935608, 2.0592730045318604, 1.6050796508789062, 1.7893290519714355, 2.5047709941864014, 3.3559494018554688, 2.657463788986206, 3.2873077392578125, 2.467412233352661, 2.0810811519622803, 2.456913709640503, 1.9378464221954346, 2.232858896255493, 1.7229676246643066, 2.724703311920166, 2.88134765625, 2.8111424446105957, 2.5387794971466064, 3.004523515701294, 1.561147928237915, 2.113420009613037, 1.533582091331482, 2.5972697734832764, 2.6002800464630127, 2.402310609817505, 2.3258302211761475, 1.5170432329177856, 2.7315192222595215, 2.299504280090332, 3.518749475479126, 2.859490394592285, 3.4380056858062744, 2.089996576309204, 1.2194043397903442, 1.2241065502166748, 2.0012311935424805, 2.733207941055298, 3.3365447521209717, 2.250166654586792, 1.6142069101333618, 3.3049490451812744, 1.3613324165344238, 3.0273938179016113, 2.322556257247925, 2.3258249759674072, 3.1546781063079834, 3.082449436187744, 3.5575449466705322, 2.493809461593628, 3.427929401397705, 1.8785370588302612, 1.583200454711914, 2.080164670944214, 3.2170674800872803, 1.033105731010437, 2.113266944885254, 2.077721357345581, 2.1769087314605713, 2.688490152359009, 2.4166579246520996, 3.1492161750793457, 2.9941306114196777, 1.8685353994369507, 1.1520304679870605, 2.4054441452026367, 2.7724568843841553, 2.034297227859497, 2.5374763011932373, 1.3330113887786865, 1.0965449810028076, 3.2038064002990723, 1.497473955154419, 2.0346200466156006, 2.0951502323150635, 2.865229606628418, 2.699307680130005, 1.3834149837493896, 2.1743195056915283, 1.6928564310073853, 2.828073263168335, 2.0100135803222656, 1.2808585166931152, 3.58377742767334, 1.3803236484527588, 1.9780139923095703, 0.729354739189148, 3.1625750064849854, 2.5120770931243896, 0.43222421407699585, 2.734494686126709, 2.6161372661590576, 1.7855305671691895, 2.5925323963165283, 2.69016170501709, 1.4435584545135498, 3.138392686843872, 3.0969879627227783, 2.30969500541687, 1.4527089595794678, 2.2286691665649414, 1.3700370788574219, 2.5567893981933594, 0.8122043013572693, 1.967794418334961, 1.747469186782837, 2.8941447734832764, 1.8959866762161255, 2.5589821338653564, 1.8241221904754639, 2.263030767440796, 2.2577064037323, 3.006815195083618, 1.671786904335022, 2.941882848739624, 3.2274062633514404, 2.119119167327881, 2.1256167888641357, 1.531799077987671, 0.9762893915176392, 1.3377456665039062, 2.2930498123168945, 2.6915171146392822, 1.571720838546753, 2.5954506397247314, 2.054884195327759, 1.782454490661621, 2.220146894454956, 3.385282278060913, 3.8847172260284424, 2.5932834148406982, 1.5403814315795898, 2.7333250045776367, 2.1724181175231934, 2.303097724914551, 3.4001333713531494, 2.1460626125335693, 1.6561405658721924, 1.5355093479156494, 1.81447172164917, 2.778587818145752, 1.059356927871704, 1.0646231174468994, 0.48029056191444397, 1.9598426818847656, 2.0391697883605957, 1.8404573202133179, 0.8604139685630798, 1.4294482469558716, 2.987920045852661, 1.6589546203613281, 3.310697317123413, 3.2078001499176025, 1.7015910148620605, 2.3720333576202393, 2.284165143966675, 1.4131944179534912, 3.0408642292022705, 3.3361058235168457, 2.798555374145508, 2.3631813526153564, 1.3748292922973633, 1.883569598197937, 1.2574471235275269, 2.893428087234497, 2.454798460006714, 1.8791697025299072, 1.7336957454681396, 1.804945707321167, 2.5795023441314697, 2.220616340637207, 1.6792851686477661, 2.0654494762420654, 2.026089668273926, 1.3571745157241821, 3.4157276153564453, 2.8497016429901123, 2.5846829414367676, 2.491915464401245, 3.35591721534729, 0.9465303421020508, 3.384166955947876, 1.3310532569885254, 2.2406351566314697, 1.0069360733032227, 1.7204632759094238, 1.781381607055664, 0.9237425923347473, 2.7493274211883545, 1.768798589706421, 1.2680566310882568, 4.042766571044922, 2.741102933883667, 2.739499568939209, 3.18485689163208, 2.8587870597839355, 3.0088002681732178, 1.895416021347046, 1.81614089012146, 1.2602949142456055, 2.652085065841675, 1.6139229536056519, 2.2387712001800537, 1.2495061159133911, 1.979046106338501, 3.0296714305877686, 1.487961769104004, 1.5356419086456299, 1.8598930835723877, 3.1642723083496094, 3.146505355834961, 3.027047872543335, 2.9896368980407715, 1.7419610023498535, 2.429516553878784, 3.170978307723999, 1.5618165731430054, 1.8231334686279297, 1.7787210941314697, 1.522687315940857, 2.3381338119506836, 1.4089117050170898, 1.6986842155456543, 1.2668747901916504, 2.9988973140716553, 2.5679516792297363, 2.6511142253875732, 2.518845558166504, 2.9224565029144287, 3.3229682445526123, 2.122490406036377, 1.9257214069366455, 1.3676787614822388, 1.695976972579956, 1.772209644317627, 2.5520565509796143, 2.1530065536499023, 1.0152827501296997, 1.693786382675171, 2.3586668968200684, 1.2952189445495605, 1.9340314865112305, 2.8161087036132812, 1.8181954622268677, 0.7734366655349731, 2.1227834224700928, 2.7032971382141113, 1.4315011501312256, 1.5781519412994385, 0.7775930762290955, 1.6754496097564697, 2.1249654293060303, 2.9686343669891357, 1.6433929204940796, 3.1063594818115234, 1.0673288106918335, 2.131554365158081, 2.526130437850952, 2.5456204414367676, 1.310762643814087, 1.090785026550293, 1.4600040912628174, 1.8967359066009521, 2.2911975383758545, 1.3211232423782349, 1.8437831401824951, 1.7957019805908203, 2.4691898822784424, 1.553450584411621, 4.128464221954346, 3.523902654647827, 3.4994394779205322, 2.642235517501831, 2.711082696914673, 2.821682929992676, 2.5182487964630127, 1.7395774126052856, 1.3690783977508545, 2.579331398010254, 1.0174108743667603, 1.8552484512329102, 2.408057451248169, 3.063359022140503, 1.432357668876648, 3.1424477100372314, 1.7379872798919678, 1.066511869430542, 1.1370084285736084, 1.8797637224197388, 2.3644919395446777, 0.9219732880592346, 3.041522264480591, 1.7713268995285034, 3.1014821529388428, 2.9535412788391113, 2.2485506534576416, 2.3403098583221436, 2.711595296859741, 1.6465309858322144, 3.0099291801452637, 2.154942274093628, 1.9962069988250732, 1.2751802206039429, 2.7102482318878174, 1.9721903800964355, 1.7286978960037231, 1.6108478307724, 1.1871907711029053, 1.7613260746002197, 1.7354695796966553, 2.507479429244995, 1.6416943073272705, 2.129654884338379, 2.737567663192749, 0.7662279605865479, 2.7136313915252686, 1.4551043510437012, 0.8828736543655396, 1.6791917085647583, 2.819636583328247, 1.8522157669067383, 2.6507465839385986, 1.8755486011505127, 2.16410231590271, 2.599494695663452, 2.1781692504882812, 1.631186604499817, 1.3714591264724731, 1.098455786705017, 3.4853575229644775, 3.8865270614624023, 2.8115203380584717, 3.1612517833709717, 3.5239672660827637, 4.266568183898926, 3.373809337615967, 3.6869847774505615, 3.3480818271636963, 3.324667453765869, 1.6267178058624268, 4.371267318725586, 4.084469318389893, 3.69865345954895, 3.583940267562866, 1.1411734819412231, 3.3021397590637207, 3.3136637210845947, 3.116044521331787, 2.3276240825653076, 4.021718502044678, 3.942639112472534, 3.131141424179077, 3.78287672996521, 3.582428216934204, 3.6762590408325195, 3.15958571434021, 3.146876096725464, 3.6341683864593506, 3.804180860519409, 3.595999002456665, 2.5246641635894775, 3.077632188796997, 3.9715137481689453, 3.225229024887085, 3.985239267349243, 4.088001251220703, 3.081392526626587, 4.558755874633789, 4.185262680053711, 3.748304605484009, 3.5616495609283447, 3.9948666095733643, 4.10636568069458, 3.4106404781341553, 2.9191951751708984, 4.019347667694092, 3.918344736099243, 3.311201810836792, 3.527902364730835, 3.127454996109009, 4.57519006729126, 3.8075270652770996, 3.06719708442688, 3.396087884902954, 3.222486972808838, 2.9178998470306396, 3.990612268447876, 3.8752753734588623, 3.10023832321167, 2.517028570175171, 4.011193752288818, 3.270801544189453, 3.8775126934051514, 3.0979158878326416, 3.8291666507720947, 3.7397685050964355, 3.5200819969177246, 1.057457447052002, 2.193624973297119, 3.044696092605591, 2.87585711479187, 3.724900007247925, 3.716313600540161, 4.352687358856201, 3.8253324031829834, 2.9211394786834717, 3.5683069229125977, 2.144599676132202, 1.2564291954040527, 4.418914318084717, 3.1729166507720947, 2.8222880363464355, 3.9651808738708496, 3.4891035556793213, 4.008902549743652, 3.8797919750213623, 3.697667121887207, 3.9141759872436523, 3.4779107570648193, 2.5185272693634033, 2.742805242538452, 4.3674845695495605, 3.6410772800445557, 4.0960164070129395, 3.398906946182251, 3.1556949615478516, 3.4520103931427, 3.4862351417541504, 3.7273716926574707, 1.0914713144302368, 4.1268534660339355, 2.526942014694214, 2.096583604812622, 1.0471396446228027, 2.6220476627349854, 3.472968578338623, 2.180732011795044, 3.909893035888672, 3.2716267108917236, 3.8965070247650146, 3.6917762756347656, 3.6798996925354004, 3.5891482830047607, 3.7091784477233887, 2.8587255477905273, 3.1929662227630615, 3.3214263916015625, 3.5722222328186035, 4.105366230010986, 2.591391086578369, 2.789128065109253, 3.09285044670105, 4.085590839385986, 2.603477716445923, 3.0005271434783936, 3.6096460819244385, 4.319613933563232, 2.9084205627441406, 4.182709217071533, 3.8784339427948, 1.63625168800354, 3.1654937267303467, 3.681391954421997, 4.0919189453125, 3.0469164848327637, 3.901712417602539, 1.698776125907898, 2.1043612957000732, 4.38751745223999, 3.744187116622925, 2.707010269165039, 4.096773624420166, 3.630894422531128, 4.249382019042969, 3.5307230949401855, 2.901348352432251, 2.3861424922943115, 1.6878536939620972, 3.7445714473724365, 3.2533276081085205, 4.531707286834717, 3.184682607650757, 3.4987313747406006, 3.80009388923645, 2.6679205894470215, 3.9394662380218506, 2.8677685260772705, 3.5198733806610107, 3.2431161403656006, 4.098721504211426, 3.537336587905884, 3.5793066024780273, 3.4921603202819824, 3.753253221511841, 3.5612590312957764, 3.2130115032196045, 3.2778587341308594, 4.1160712242126465, 3.2760705947875977, 3.867865800857544, 2.7146103382110596, 3.375978708267212, 4.0001349449157715, 4.279506206512451, 3.672565460205078, 4.181159496307373, 3.223646640777588, 4.356719017028809, 3.6398584842681885, 3.7870805263519287, 1.7536710500717163, 3.4514176845550537, 2.974785566329956, 3.26245379447937, 4.309142589569092, 3.3585445880889893, 2.2956507205963135, 3.7228691577911377, 3.0308873653411865, 4.017061710357666, 4.135494709014893, 3.7096400260925293, 3.377965211868286, 3.868150234222412, 3.6727826595306396, 3.955432176589966, 3.7737793922424316, 2.773301362991333, 4.184505939483643, 3.4834237098693848, 3.954754114151001, 3.7586591243743896, 2.225085973739624, 4.011172294616699, 4.099438190460205, 3.220275402069092, 0.9563226103782654, 3.733246088027954, 3.9404518604278564, 2.0690817832946777, 3.5967206954956055, 3.7692720890045166, 3.993608236312866, 2.3685147762298584, 4.047364234924316, 4.307677268981934, 4.7334418296813965, 3.21227765083313, 3.738865613937378, 4.454609394073486, 1.9718149900436401, 4.216540336608887, 1.2689083814620972, 0.48906150460243225, 4.816674709320068, 3.7868988513946533, 1.339495062828064, 4.532032012939453, 1.9387511014938354, 3.085062265396118, 2.4119746685028076, 1.279048204421997, 4.805697441101074, 3.907646656036377, 0.5046525001525879, 1.8610246181488037, 4.159181118011475, 1.618883728981018, 2.6896579265594482, 2.04736590385437, 2.9977643489837646, 0.07772776484489441, 3.8029110431671143, 0.911912202835083, 2.3547439575195312, 1.079477310180664, 2.510418176651001, 2.843278646469116, 3.447214365005493, 2.1058762073516846, 3.7027781009674072, 4.820074558258057, 4.384733200073242, 4.699190139770508, 3.6574819087982178, 1.3987233638763428, 1.4721883535385132, 0.7775132656097412, 1.5421035289764404, 3.848310947418213, 1.5656412839889526, 2.5501978397369385, 2.9160234928131104, 1.4491169452667236, 0.9072403907775879, 1.1175816059112549, 4.502264499664307, 3.9981586933135986, 4.197209358215332, 2.9409422874450684, 4.005847454071045, 3.796816110610962, 2.782438278198242, 2.7287425994873047, 3.1240220069885254, 3.1060750484466553, 2.969263792037964, 2.3006842136383057, 1.0213840007781982, 2.5870981216430664, 3.544971466064453, 4.3163323402404785, 4.006761074066162, 1.5898051261901855, 1.1052844524383545, 0.597686231136322, 3.6905159950256348, 2.837252378463745, 4.134679317474365, 3.144198179244995, 3.548229932785034, 3.6341116428375244, 4.474788188934326, 3.5696163177490234, 0.6314138770103455, 3.1437630653381348, 0.21768487989902496, 2.716078996658325, 2.6165192127227783, 3.370713949203491, 1.1333866119384766, 3.252734899520874, 2.5071351528167725, 3.2509753704071045, 4.351325035095215, 2.1304500102996826, 4.834465980529785, 1.6397438049316406, 3.656625270843506, 4.179050922393799, 3.446464776992798, 0.7241423726081848, 1.282979965209961, 1.4860398769378662, 1.8374407291412354, 3.9657816886901855, 2.302959680557251, 0.2764548659324646, 3.020024061203003, 2.0180540084838867, 0.7722868919372559, 3.8498375415802, 0.5464186668395996, 2.378264904022217, 0.374484658241272, 3.2630538940429688, 0.13047118484973907, 4.894762992858887, 1.0539461374282837, 4.520473957061768, 2.5159754753112793, 4.932023048400879, 4.044747829437256, 0.5224081873893738, 3.1377947330474854, 2.110215425491333, 4.230230808258057, 3.793083667755127, 1.5374054908752441, 4.230461597442627, 1.1427040100097656, 2.079801321029663, 1.2932991981506348, 3.288308620452881, 3.186271905899048, 1.8072929382324219, 3.1530816555023193, 4.032928466796875, 2.24214243888855, 2.031496286392212, 2.652290105819702, 3.373603105545044, 0.9203230142593384, 4.344527721405029, 4.257948398590088, 2.526681661605835, 4.749520778656006, 0.21622031927108765, 4.274327754974365, 2.629260540008545, 0.9817190170288086, 4.148682117462158, 2.6683192253112793, 2.8288257122039795, 4.399178981781006, 2.388913869857788, 2.09065318107605, 0.8813363909721375, 2.63423752784729, 2.681864023208618, 4.198167324066162, 2.5258209705352783, 0.2116391509771347, 0.2482755482196808, 3.5087697505950928, 4.770411491394043, 1.228097677230835, 3.746103525161743, 3.813230276107788, 0.22903093695640564, 4.701204776763916, 3.3656325340270996, 1.6457003355026245, 1.3053816556930542, 0.04780619964003563, 2.0247464179992676, 4.557525634765625, 0.40976545214653015, 0.9991333484649658, 1.2625190019607544, 4.2843146324157715, 4.3732523918151855, 1.0569288730621338, 3.3848750591278076, 0.9187212586402893, 0.4987865388393402, 4.486860752105713, 3.950536012649536, 2.9888501167297363, 0.8773539066314697, 0.236017644405365, 2.462407350540161, 3.152488946914673, 0.10695049166679382, 1.396593451499939, 0.26900163292884827, 1.8687732219696045, 0.5808807015419006, 0.724673330783844, 0.5587760210037231, 2.6382856369018555, 2.091080904006958, 4.540224552154541, 3.047802686691284, 1.8072333335876465, 1.6234714984893799, 3.766123056411743, 1.097754955291748, 0.1299261450767517, 0.1761685609817505, 0.12438392639160156, 0.13142146170139313, 1.9501628875732422, 0.5709967613220215, 3.5401108264923096, 0.2805880904197693, 0.38812491297721863, 2.5602643489837646, 4.047485828399658, 3.7545807361602783, 0.45663562417030334, 0.3955705463886261, 1.964401125907898, 0.7354254126548767, 1.4115345478057861, 1.5711336135864258, 0.6200692653656006, 0.5970773696899414, 5.0607452392578125, 1.322024941444397, 1.6455802917480469, 2.941373109817505, 3.9770472049713135, 3.112680673599243, 0.25985464453697205, 0.11964204907417297, 4.731985569000244, 4.013974666595459, 3.1663339138031006, 4.581935882568359, 4.4286789894104, 4.444067478179932, 2.8654959201812744, 1.8979032039642334, 3.5925238132476807, 1.0107598304748535, 4.116321563720703, 2.622354030609131, 3.6956050395965576, 3.6642167568206787, 3.938272714614868, 2.5276403427124023, 3.608003854751587, 1.1186381578445435, 3.815798044204712, 0.722634494304657, 1.431228518486023, 0.5237283110618591, 0.7154967188835144]\n"
          ]
        }
      ],
      "source": [
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5.0, 4.75, 5.0, 2.4000000953674316, 2.75, 2.615000009536743, 5.0, 2.3329999446868896, 3.75, 5.0, 3.200000047683716, 1.5829999446868896, 5.0, 5.0, 4.908999919891357, 0.800000011920929, 2.4000000953674316, 5.0, 4.0, 0.6359999775886536, 3.0, 1.7139999866485596, 3.200000047683716, 2.1670000553131104, 1.0, 1.9170000553131104, 4.25, 3.0, 1.0, 0.6000000238418579, 2.5999999046325684, 5.0, 4.599999904632568, 5.0, 4.800000190734863, 3.799999952316284, 5.0, 5.0, 4.199999809265137, 1.399999976158142, 3.5999999046325684, 2.799999952316284, 1.600000023841858, 3.0, 1.399999976158142, 0.25, 0.25, 0.0, 4.0, 4.5, 0.5, 3.799999952316284, 4.800000190734863, 5.0, 0.25, 1.2000000476837158, 0.6000000238418579, 0.800000011920929, 3.799999952316284, 0.0, 3.5, 4.5, 2.799999952316284, 3.799999952316284, 3.799999952316284, 0.0, 4.0, 4.25, 2.812000036239624, 4.25, 3.0, 1.0, 3.75, 0.0, 0.4000000059604645, 4.0, 2.799999952316284, 3.75, 1.1540000438690186, 2.75, 2.799999952316284, 0.0, 0.0, 3.4000000953674316, 5.0, 0.800000011920929, 2.25, 2.75, 4.5, 2.5999999046325684, 3.799999952316284, 2.799999952316284, 1.600000023841858, 0.0, 1.3329999446868896, 2.5999999046325684, 0.6000000238418579, 0.0, 0.0, 3.0, 0.0, 0.20000000298023224, 3.200000047683716, 2.0, 0.0, 3.0, 3.0910000801086426, 2.75, 1.2000000476837158, 0.5, 0.25, 0.0, 0.0, 0.5, 0.0, 0.6000000238418579, 4.0, 3.4000000953674316, 0.0, 0.0, 1.0, 0.0, 2.4000000953674316, 0.4000000059604645, 1.7999999523162842, 2.5, 0.08299999684095383, 0.0, 1.0, 4.5, 0.0, 3.5999999046325684, 5.0, 5.0, 5.0, 5.0, 3.799999952316284, 5.0, 2.5, 3.8239998817443848, 1.600000023841858, 5.0, 5.0, 2.375, 5.0, 1.25, 1.25, 3.0, 2.1670000553131104, 1.399999976158142, 4.0, 3.5999999046325684, 4.85699987411499, 0.4000000059604645, 0.20000000298023224, 1.2999999523162842, 1.2000000476837158, 1.2000000476837158, 4.400000095367432, 1.7999999523162842, 1.399999976158142, 0.4000000059604645, 4.400000095367432, 3.25, 0.777999997138977, 2.25, 2.75, 2.0, 0.8330000042915344, 3.75, 1.7999999523162842, 4.333000183105469, 4.214000225067139, 0.0, 3.4000000953674316, 4.199999809265137, 0.800000011920929, 4.199999809265137, 2.75, 2.200000047683716, 1.2000000476837158, 1.0, 1.5329999923706055, 0.20000000298023224, 3.0, 3.799999952316284, 0.75, 3.0, 0.4000000059604645, 3.4000000953674316, 3.799999952316284, 1.399999976158142, 3.4000000953674316, 3.200000047683716, 2.3329999446868896, 0.800000011920929, 0.0, 3.0, 3.75, 0.0, 0.0, 0.0, 0.5, 0.5, 0.4000000059604645, 0.75, 2.5, 1.7999999523162842, 2.5, 0.5, 0.4000000059604645, 1.2000000476837158, 2.75, 0.4000000059604645, 0.6000000238418579, 0.800000011920929, 3.0, 0.4000000059604645, 3.200000047683716, 1.25, 5.0, 0.20000000298023224, 3.0, 1.399999976158142, 3.799999952316284, 2.75, 0.0, 2.799999952316284, 0.0, 0.0, 2.5999999046325684, 0.10000000149011612, 0.0, 2.0, 3.75, 3.0999999046325684, 2.691999912261963, 4.428999900817871, 5.0, 0.0, 0.800000011920929, 1.3329999446868896, 0.0, 0.6000000238418579, 0.0, 0.0, 0.0, 3.4000000953674316, 2.200000047683716, 0.0, 3.5999999046325684, 0.0, 1.399999976158142, 2.200000047683716, 3.799999952316284, 2.5999999046325684, 0.4000000059604645, 4.0, 2.25, 3.200000047683716, 0.0, 0.4000000059604645, 3.799999952316284, 0.800000011920929, 4.199999809265137, 3.5999999046325684, 2.0, 4.199999809265137, 3.4000000953674316, 3.0, 3.4000000953674316, 3.5999999046325684, 3.0, 2.4000000953674316, 2.0, 4.0, 0.0, 4.0, 4.199999809265137, 0.800000011920929, 2.200000047683716, 3.799999952316284, 3.200000047683716, 4.400000095367432, 0.0, 1.399999976158142, 2.5999999046325684, 0.4000000059604645, 0.0, 3.200000047683716, 4.599999904632568, 0.0, 0.6000000238418579, 1.2000000476837158, 3.799999952316284, 3.200000047683716, 2.5999999046325684, 5.0, 0.0, 3.200000047683716, 4.0, 1.2000000476837158, 4.400000095367432, 3.4000000953674316, 4.400000095367432, 0.800000011920929, 4.599999904632568, 4.199999809265137, 0.0, 3.200000047683716, 0.0, 1.399999976158142, 3.200000047683716, 1.7999999523162842, 1.7999999523162842, 0.0, 3.799999952316284, 3.799999952316284, 4.0, 0.0, 3.200000047683716, 1.600000023841858, 3.5999999046325684, 1.0, 3.5999999046325684, 2.0, 3.5999999046325684, 4.0, 3.5999999046325684, 3.200000047683716, 2.799999952316284, 2.4000000953674316, 3.5999999046325684, 1.7999999523162842, 0.20000000298023224, 3.4000000953674316, 1.2000000476837158, 1.399999976158142, 2.5999999046325684, 2.4000000953674316, 0.0, 4.0, 3.0, 4.0, 4.0, 2.0, 4.0, 2.200000047683716, 2.200000047683716, 3.4000000953674316, 3.200000047683716, 4.0, 3.200000047683716, 3.799999952316284, 3.0, 4.0, 2.200000047683716, 3.4000000953674316, 3.799999952316284, 0.0, 0.0, 4.800000190734863, 3.0, 0.800000011920929, 4.199999809265137, 4.199999809265137, 0.0, 4.199999809265137, 4.599999904632568, 5.0, 0.0, 3.200000047683716, 2.5999999046325684, 2.200000047683716, 4.599999904632568, 5.0, 4.599999904632568, 4.599999904632568, 4.800000190734863, 0.6000000238418579, 0.20000000298023224, 2.200000047683716, 4.800000190734863, 1.2000000476837158, 2.200000047683716, 4.599999904632568, 2.5999999046325684, 3.5999999046325684, 0.800000011920929, 2.4000000953674316, 4.800000190734863, 5.0, 0.4000000059604645, 3.200000047683716, 1.7999999523162842, 3.200000047683716, 3.4000000953674316, 4.800000190734863, 4.199999809265137, 2.5999999046325684, 2.200000047683716, 1.399999976158142, 2.5999999046325684, 1.2000000476837158, 1.7999999523162842, 4.0, 0.4000000059604645, 0.0, 0.4000000059604645, 3.0, 4.0, 1.399999976158142, 0.4000000059604645, 2.4000000953674316, 2.799999952316284, 4.599999904632568, 4.199999809265137, 3.200000047683716, 1.2000000476837158, 1.7999999523162842, 2.5999999046325684, 1.600000023841858, 0.0, 3.799999952316284, 4.0, 4.800000190734863, 2.4000000953674316, 5.0, 0.0, 3.5999999046325684, 2.4000000953674316, 4.199999809265137, 0.20000000298023224, 4.199999809265137, 3.200000047683716, 0.6000000238418579, 1.600000023841858, 0.0, 1.399999976158142, 4.800000190734863, 4.0, 3.0, 0.20000000298023224, 1.600000023841858, 4.400000095367432, 4.800000190734863, 0.4000000059604645, 2.4000000953674316, 3.0, 2.0, 4.599999904632568, 1.0, 5.0, 1.399999976158142, 0.0, 3.0, 0.6000000238418579, 0.0, 2.5999999046325684, 1.2000000476837158, 5.0, 3.4000000953674316, 0.800000011920929, 0.800000011920929, 1.0, 0.0, 0.0, 2.0, 2.4000000953674316, 2.200000047683716, 3.799999952316284, 2.200000047683716, 4.599999904632568, 2.799999952316284, 2.0, 4.199999809265137, 4.5, 0.0, 1.7999999523162842, 4.199999809265137, 4.199999809265137, 1.399999976158142, 3.5999999046325684, 0.6000000238418579, 5.0, 0.800000011920929, 0.0, 0.0, 1.7999999523162842, 0.800000011920929, 2.799999952316284, 0.0, 0.800000011920929, 1.7999999523162842, 1.399999976158142, 1.7999999523162842, 2.4000000953674316, 1.600000023841858, 3.799999952316284, 4.800000190734863, 0.20000000298023224, 1.0, 0.4000000059604645, 0.4000000059604645, 4.0, 3.4000000953674316, 3.0, 1.7999999523162842, 2.200000047683716, 3.5999999046325684, 0.4000000059604645, 0.20000000298023224, 3.799999952316284, 4.199999809265137, 1.2000000476837158, 3.4000000953674316, 2.4000000953674316, 3.200000047683716, 2.200000047683716, 4.400000095367432, 2.5999999046325684, 0.20000000298023224, 3.200000047683716, 2.200000047683716, 5.0, 5.0, 2.200000047683716, 0.4000000059604645, 1.0, 2.200000047683716, 2.0, 1.2000000476837158, 4.0, 1.0, 4.199999809265137, 3.4000000953674316, 4.0, 4.800000190734863, 0.4000000059604645, 4.400000095367432, 0.4000000059604645, 0.0, 3.0, 4.599999904632568, 0.4000000059604645, 1.399999976158142, 3.0, 1.600000023841858, 2.4000000953674316, 0.0, 1.600000023841858, 1.600000023841858, 0.20000000298023224, 2.200000047683716, 2.0, 0.4000000059604645, 4.800000190734863, 2.0, 0.20000000298023224, 3.799999952316284, 2.200000047683716, 2.0, 4.0, 4.599999904632568, 1.399999976158142, 0.800000011920929, 0.6000000238418579, 1.2000000476837158, 1.600000023841858, 0.0, 0.6000000238418579, 3.0, 2.4000000953674316, 0.6000000238418579, 4.599999904632568, 0.6000000238418579, 4.800000190734863, 5.0, 0.20000000298023224, 0.6000000238418579, 0.800000011920929, 3.5999999046325684, 0.0, 4.400000095367432, 4.599999904632568, 2.799999952316284, 4.0, 1.0, 5.0, 0.4000000059604645, 0.800000011920929, 0.4000000059604645, 0.0, 1.600000023841858, 1.600000023841858, 3.5999999046325684, 3.0, 2.799999952316284, 0.20000000298023224, 1.0, 1.399999976158142, 2.799999952316284, 3.0, 1.600000023841858, 2.799999952316284, 4.0, 0.800000011920929, 2.0, 4.400000095367432, 4.599999904632568, 2.4000000953674316, 4.400000095367432, 0.800000011920929, 3.799999952316284, 1.399999976158142, 2.799999952316284, 3.0, 1.600000023841858, 1.0, 2.5999999046325684, 1.600000023841858, 0.20000000298023224, 3.0, 3.0, 3.200000047683716, 3.0, 0.6000000238418579, 1.0, 0.6000000238418579, 0.800000011920929, 2.4000000953674316, 1.0, 0.20000000298023224, 0.6000000238418579, 3.5999999046325684, 0.20000000298023224, 0.20000000298023224, 0.0, 0.0, 1.2000000476837158, 0.0, 0.4000000059604645, 0.4000000059604645, 3.0, 4.400000095367432, 3.200000047683716, 3.4000000953674316, 3.4000000953674316, 3.5999999046325684, 0.800000011920929, 0.800000011920929, 1.0, 1.600000023841858, 1.0, 3.200000047683716, 2.0, 0.0, 0.6000000238418579, 0.0, 1.399999976158142, 2.0, 2.5999999046325684, 4.199999809265137, 2.799999952316284, 0.0, 0.0, 2.0, 0.0, 2.5999999046325684, 2.5999999046325684, 0.0, 2.5999999046325684, 0.0, 1.600000023841858, 0.0, 0.6000000238418579, 1.7999999523162842, 3.799999952316284, 1.600000023841858, 3.200000047683716, 2.5999999046325684, 3.0, 1.399999976158142, 1.600000023841858, 1.600000023841858, 1.0, 1.7999999523162842, 1.7999999523162842, 1.399999976158142, 2.5999999046325684, 2.0, 1.0, 1.399999976158142, 0.6000000238418579, 1.399999976158142, 0.4000000059604645, 1.7999999523162842, 4.0, 0.6000000238418579, 2.799999952316284, 2.5999999046325684, 2.5999999046325684, 0.6000000238418579, 4.400000095367432, 2.200000047683716, 1.0, 0.0, 1.600000023841858, 1.7999999523162842, 3.799999952316284, 3.0, 1.399999976158142, 3.0, 0.4000000059604645, 3.5999999046325684, 3.4000000953674316, 2.799999952316284, 4.199999809265137, 4.199999809265137, 1.7999999523162842, 1.0, 2.0, 1.0, 1.600000023841858, 2.5999999046325684, 3.200000047683716, 0.4000000059604645, 0.0, 0.0, 2.200000047683716, 1.5, 0.6000000238418579, 4.0, 1.7999999523162842, 0.6000000238418579, 1.600000023841858, 2.4000000953674316, 1.0, 2.0, 1.600000023841858, 0.20000000298023224, 0.20000000298023224, 2.0, 0.0, 3.200000047683716, 1.0, 2.0, 2.4000000953674316, 0.0, 1.600000023841858, 3.799999952316284, 1.2000000476837158, 2.0, 1.0, 2.0, 0.6000000238418579, 0.0, 0.0, 3.4000000953674316, 2.5999999046325684, 0.4000000059604645, 2.799999952316284, 2.0, 1.600000023841858, 1.399999976158142, 2.200000047683716, 0.6000000238418579, 2.5999999046325684, 3.0, 2.200000047683716, 1.7999999523162842, 0.6000000238418579, 0.0, 1.600000023841858, 0.800000011920929, 0.20000000298023224, 1.0, 2.799999952316284, 0.6000000238418579, 2.799999952316284, 1.600000023841858, 1.399999976158142, 1.399999976158142, 3.0, 3.200000047683716, 2.799999952316284, 3.4000000953674316, 2.0, 2.799999952316284, 0.0, 0.20000000298023224, 0.800000011920929, 0.0, 4.199999809265137, 0.20000000298023224, 3.200000047683716, 0.0, 0.6000000238418579, 0.0, 4.800000190734863, 3.799999952316284, 1.7999999523162842, 0.4000000059604645, 2.5999999046325684, 1.600000023841858, 1.399999976158142, 2.0, 1.7999999523162842, 1.2000000476837158, 0.800000011920929, 2.200000047683716, 3.200000047683716, 1.399999976158142, 1.399999976158142, 0.0, 1.600000023841858, 0.800000011920929, 2.799999952316284, 1.7999999523162842, 0.6700000166893005, 2.5999999046325684, 0.4000000059604645, 2.4000000953674316, 2.799999952316284, 2.0, 3.0, 2.5999999046325684, 1.7999999523162842, 3.799999952316284, 3.0, 1.399999976158142, 1.600000023841858, 0.0, 0.6000000238418579, 0.0, 3.4000000953674316, 3.5999999046325684, 2.200000047683716, 3.0, 1.7999999523162842, 2.4000000953674316, 2.5999999046325684, 1.399999976158142, 1.600000023841858, 1.600000023841858, 1.399999976158142, 2.799999952316284, 3.799999952316284, 3.4000000953674316, 0.0, 2.0, 0.0, 3.799999952316284, 0.20000000298023224, 0.6000000238418579, 1.7999999523162842, 1.7999999523162842, 1.2000000476837158, 0.0, 1.0, 0.4000000059604645, 0.800000011920929, 3.0, 3.799999952316284, 3.200000047683716, 2.5999999046325684, 3.0, 2.5999999046325684, 1.7999999523162842, 2.5999999046325684, 0.800000011920929, 1.600000023841858, 0.6000000238418579, 2.5999999046325684, 1.2000000476837158, 0.0, 2.4000000953674316, 0.4000000059604645, 0.6000000238418579, 1.0, 2.5999999046325684, 4.0, 3.5999999046325684, 2.5999999046325684, 1.600000023841858, 4.0, 3.0, 0.6000000238418579, 0.0, 0.4000000059604645, 0.4000000059604645, 2.0, 0.6000000238418579, 0.0, 3.200000047683716, 2.799999952316284, 2.5999999046325684, 2.799999952316284, 0.20000000298023224, 3.200000047683716, 0.6000000238418579, 3.5999999046325684, 0.0, 0.0, 0.0, 0.0, 2.200000047683716, 0.0, 0.4000000059604645, 0.0, 2.5999999046325684, 0.800000011920929, 0.4000000059604645, 3.0, 2.200000047683716, 0.6000000238418579, 3.4000000953674316, 2.200000047683716, 1.7999999523162842, 0.800000011920929, 0.0, 1.600000023841858, 2.799999952316284, 1.6699999570846558, 1.7999999523162842, 2.799999952316284, 0.0, 0.6000000238418579, 1.7999999523162842, 0.4000000059604645, 0.20000000298023224, 0.0, 1.399999976158142, 0.0, 0.6000000238418579, 0.800000011920929, 0.20000000298023224, 2.200000047683716, 1.7999999523162842, 0.20000000298023224, 4.0, 4.199999809265137, 4.400000095367432, 3.0, 2.5999999046325684, 0.6000000238418579, 3.200000047683716, 2.5999999046325684, 1.2000000476837158, 3.0, 0.0, 2.5999999046325684, 1.2000000476837158, 2.799999952316284, 0.20000000298023224, 1.7999999523162842, 0.0, 0.0, 3.0, 0.0, 2.4000000953674316, 0.4000000059604645, 2.5999999046325684, 0.0, 3.5999999046325684, 1.7999999523162842, 1.600000023841858, 2.0, 2.0, 0.0, 2.5999999046325684, 0.4000000059604645, 0.0, 0.20000000298023224, 1.7999999523162842, 1.600000023841858, 1.0, 0.6000000238418579, 1.0, 2.799999952316284, 2.5999999046325684, 3.4000000953674316, 1.600000023841858, 4.0, 1.7999999523162842, 0.20000000298023224, 0.0, 1.399999976158142, 0.0, 1.2000000476837158, 3.5999999046325684, 3.0, 1.2000000476837158, 1.7999999523162842, 0.800000011920929, 1.2000000476837158, 0.0, 3.200000047683716, 0.20000000298023224, 0.800000011920929, 3.5999999046325684, 5.0, 1.399999976158142, 2.5999999046325684, 2.0, 3.75, 4.0, 3.75, 3.200000047683716, 4.0, 2.4000000953674316, 4.400000095367432, 4.199999809265137, 3.200000047683716, 3.0, 0.800000011920929, 3.200000047683716, 3.799999952316284, 3.0, 2.4000000953674316, 5.0, 3.799999952316284, 3.200000047683716, 3.799999952316284, 3.799999952316284, 3.75, 3.200000047683716, 3.4000000953674316, 4.599999904632568, 3.3329999446868896, 3.5, 1.75, 3.0, 4.400000095367432, 3.799999952316284, 3.0, 4.5, 2.200000047683716, 4.0, 4.599999904632568, 3.5, 4.400000095367432, 5.0, 5.0, 3.200000047683716, 2.200000047683716, 3.25, 3.75, 3.200000047683716, 4.400000095367432, 2.5999999046325684, 3.5, 3.75, 3.5, 4.25, 2.4000000953674316, 3.25, 3.799999952316284, 3.4000000953674316, 3.5, 2.4000000953674316, 3.799999952316284, 3.799999952316284, 4.0, 3.4000000953674316, 2.799999952316284, 3.75, 4.400000095367432, 1.0, 2.799999952316284, 3.25, 1.75, 1.75, 5.0, 3.5999999046325684, 3.0, 2.799999952316284, 3.5999999046325684, 3.0, 1.0, 5.0, 3.5, 3.691999912261963, 4.0, 3.5, 4.75, 3.200000047683716, 2.5999999046325684, 3.0, 3.25, 2.200000047683716, 3.5, 4.75, 3.200000047683716, 3.25, 3.75, 3.0, 2.5999999046325684, 3.25, 3.4000000953674316, 1.3329999446868896, 5.0, 4.0, 3.5999999046325684, 0.800000011920929, 3.5, 3.0, 1.6670000553131104, 3.75, 2.25, 4.199999809265137, 3.5999999046325684, 3.4000000953674316, 3.0, 3.25, 3.5999999046325684, 3.0, 1.75, 3.25, 4.400000095367432, 2.799999952316284, 2.75, 3.0, 3.5999999046325684, 3.0, 2.0, 3.0, 4.400000095367432, 2.4000000953674316, 4.400000095367432, 4.400000095367432, 2.5999999046325684, 3.0, 4.5, 3.0, 1.75, 3.5999999046325684, 2.5, 1.5, 5.0, 3.75, 1.75, 4.400000095367432, 3.0, 3.799999952316284, 3.25, 2.75, 3.4000000953674316, 3.200000047683716, 3.0, 3.25, 5.0, 3.3329999446868896, 3.5999999046325684, 3.799999952316284, 3.0, 4.25, 3.5, 3.200000047683716, 2.4000000953674316, 4.0, 4.666999816894531, 3.3329999446868896, 3.0, 3.25, 4.0, 3.5, 2.0, 3.5999999046325684, 3.4000000953674316, 3.5, 2.4000000953674316, 3.5, 2.799999952316284, 4.800000190734863, 3.0, 3.4000000953674316, 3.25, 4.75, 3.5999999046325684, 4.199999809265137, 2.0, 3.799999952316284, 1.600000023841858, 3.25, 5.0, 3.4170000553131104, 2.75, 4.111000061035156, 3.5999999046325684, 3.5999999046325684, 4.333000183105469, 3.200000047683716, 2.25, 4.5, 2.75, 4.0, 3.4000000953674316, 2.200000047683716, 3.25, 3.6670000553131104, 4.75, 3.5, 3.0, 3.25, 3.5999999046325684, 3.799999952316284, 1.0, 3.3329999446868896, 4.25, 1.75, 2.75, 3.25, 2.5, 3.0910000801086426, 4.400000095367432, 4.800000190734863, 5.0, 3.0, 3.200000047683716, 4.400000095367432, 1.7999999523162842, 4.0, 1.0, 0.6000000238418579, 4.599999904632568, 1.7999999523162842, 1.2000000476837158, 4.599999904632568, 1.399999976158142, 3.200000047683716, 2.799999952316284, 0.800000011920929, 3.799999952316284, 3.799999952316284, 4.199999809265137, 1.600000023841858, 3.799999952316284, 1.399999976158142, 3.5999999046325684, 1.7999999523162842, 3.799999952316284, 0.20000000298023224, 1.600000023841858, 0.4000000059604645, 2.3333332538604736, 0.4000000059604645, 3.5999999046325684, 3.4000000953674316, 2.5999999046325684, 2.200000047683716, 2.5999999046325684, 4.800000190734863, 4.800000190734863, 4.800000190734863, 2.5999999046325684, 1.600000023841858, 2.200000047683716, 1.0, 1.2000000476837158, 3.4000000953674316, 0.6000000238418579, 3.200000047683716, 3.200000047683716, 1.7999999523162842, 1.2000000476837158, 1.600000023841858, 4.800000190734863, 3.5999999046325684, 3.799999952316284, 3.0, 3.5999999046325684, 4.400000095367432, 2.0, 2.4000000953674316, 3.200000047683716, 3.4000000953674316, 2.799999952316284, 3.200000047683716, 1.7999999523162842, 2.799999952316284, 3.4000000953674316, 4.599999904632568, 3.799999952316284, 1.600000023841858, 0.800000011920929, 1.0, 2.5999999046325684, 4.199999809265137, 4.400000095367432, 2.200000047683716, 2.5999999046325684, 2.799999952316284, 4.0, 3.200000047683716, 0.0, 1.7999999523162842, 0.0, 2.5999999046325684, 2.75, 3.200000047683716, 1.7999999523162842, 4.199999809265137, 4.199999809265137, 2.5999999046325684, 4.199999809265137, 2.0, 5.0, 0.800000011920929, 3.799999952316284, 1.600000023841858, 3.4000000953674316, 0.6000000238418579, 1.399999976158142, 3.799999952316284, 1.2000000476837158, 3.0, 2.200000047683716, 1.0, 2.200000047683716, 1.7999999523162842, 0.0, 4.199999809265137, 0.4000000059604645, 2.5999999046325684, 0.6000000238418579, 3.799999952316284, 0.0, 5.0, 1.0, 3.200000047683716, 2.799999952316284, 4.599999904632568, 3.4000000953674316, 1.2000000476837158, 4.0, 1.7999999523162842, 4.0, 2.4000000953674316, 1.7999999523162842, 3.799999952316284, 1.399999976158142, 2.5999999046325684, 1.399999976158142, 1.600000023841858, 4.800000190734863, 0.800000011920929, 3.4000000953674316, 3.200000047683716, 2.200000047683716, 1.2000000476837158, 3.0, 2.5999999046325684, 0.4000000059604645, 4.599999904632568, 4.0, 2.799999952316284, 3.799999952316284, 0.0, 3.4000000953674316, 4.599999904632568, 0.800000011920929, 3.799999952316284, 2.200000047683716, 3.200000047683716, 5.0, 1.600000023841858, 2.200000047683716, 0.800000011920929, 3.5999999046325684, 2.5, 3.0, 2.200000047683716, 0.0, 0.20000000298023224, 3.200000047683716, 5.0, 2.200000047683716, 4.400000095367432, 3.799999952316284, 1.2000000476837158, 4.599999904632568, 3.799999952316284, 1.399999976158142, 3.799999952316284, 0.0, 1.600000023841858, 5.0, 0.4000000059604645, 2.4000000953674316, 3.200000047683716, 3.799999952316284, 2.5999999046325684, 1.0, 4.0, 2.0, 2.200000047683716, 4.599999904632568, 4.400000095367432, 1.7999999523162842, 0.0, 0.0, 2.0, 3.4000000953674316, 0.20000000298023224, 2.4000000953674316, 0.0, 0.4000000059604645, 1.7999999523162842, 0.800000011920929, 0.4000000059604645, 2.200000047683716, 1.7999999523162842, 4.599999904632568, 2.200000047683716, 1.0, 1.0, 4.199999809265137, 0.4000000059604645, 0.20000000298023224, 0.0, 0.0, 0.0, 0.6000000238418579, 0.6000000238418579, 4.0, 0.0, 0.800000011920929, 3.200000047683716, 4.800000190734863, 4.199999809265137, 2.200000047683716, 1.0, 3.5999999046325684, 1.600000023841858, 3.0, 2.4000000953674316, 0.4000000059604645, 0.0, 5.0, 1.2000000476837158, 1.399999976158142, 4.0, 4.599999904632568, 3.4000000953674316, 0.4000000059604645, 0.0, 4.800000190734863, 5.0, 4.0, 4.0, 3.0, 5.0, 2.0, 3.0, 3.0, 1.0, 5.0, 2.0, 2.0, 2.0, 5.0, 3.0, 3.0, 0.0, 2.0, 0.0, 2.0, 0.0, 0.0]\n"
          ]
        }
      ],
      "source": [
        "print(labels)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
