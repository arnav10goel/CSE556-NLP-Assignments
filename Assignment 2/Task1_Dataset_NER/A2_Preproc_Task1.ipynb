{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "with open('NER_TRAIN_JUDGEMENT.json', 'r', encoding='utf-8') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# print('Number of sentences in the dataset:', len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets with an 85:15 ratio (randomly stratified)\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the split datasets into separate JSON files\n",
    "with open('train_dataset.json', 'w', encoding='utf-8') as train_file:\n",
    "    json.dump(train_data, train_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open('val_dataset.json', 'w', encoding='utf-8') as val_file:\n",
    "    json.dump(val_data, val_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# print(\"Training set size:\", len(train_data))\n",
    "# print(\"Validation set size:\", len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "# Function to tokenize text based on space\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    start = 0\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        if text[i] == ' ':\n",
    "            tokens.append([start, text[start:i], i-1])\n",
    "            start = i + 1\n",
    "        i += 1\n",
    "    tokens.append([start, text[start:i], i-1])\n",
    "    return tokens\n",
    "\n",
    "# Function to perform binary search to find token indices\n",
    "def binary_search(tokens, target_index):\n",
    "    low, high = 0, len(tokens) - 1\n",
    "    start_index = -1\n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        if tokens[mid][0] == target_index:\n",
    "            return mid\n",
    "        elif tokens[mid][0] < target_index:\n",
    "            start_index = mid\n",
    "            low = mid + 1\n",
    "        else:\n",
    "            high = mid - 1\n",
    "    return start_index\n",
    "\n",
    "\n",
    "# Function to assign BIO labels\n",
    "def assign_bio_labels(text, annotations):\n",
    "    tokens = tokenize_text(text)\n",
    "    bio_labels = ['O'] * len(tokens)\n",
    "    for annotation in annotations:\n",
    "        for result in annotation['result']:\n",
    "            start, end = result['value']['start'], result['value']['end']\n",
    "            label = result['value']['labels'][0]\n",
    "            start_index = binary_search(tokens, start)\n",
    "            end_index = binary_search(tokens, end)\n",
    "            if start_index != -1 and end_index != -1:\n",
    "                for i in range(start_index, end_index + 1):\n",
    "                    if i == start_index:\n",
    "                        bio_labels[i] = 'B_' + label\n",
    "                    else:\n",
    "                        bio_labels[i] = 'I_' + label\n",
    "            # elif start_index != -1 or end_index != -1:\n",
    "            #     print('Tokenization error')\n",
    "    return bio_labels\n",
    "\n",
    "############################################################\n",
    "# Load the train dataset\n",
    "with open('train_dataset.json', 'r', encoding='utf-8') as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "# Initialize a dictionary to store the train output\n",
    "train_output = defaultdict(dict)\n",
    "\n",
    "# Process each data instance in the train dataset\n",
    "for instance in train_data:\n",
    "    case_id = instance['id']\n",
    "    text = instance['data']['text']\n",
    "    annotations = instance['annotations']\n",
    "    bio_labels = assign_bio_labels(text, annotations)\n",
    "    # Store the text and BIO labels in the train output dictionary\n",
    "    train_output[case_id]['text'] = text\n",
    "    train_output[case_id]['labels'] = bio_labels\n",
    "\n",
    "# Save the train output to a JSON file\n",
    "with open('train_bio_labels.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(train_output, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "############################################################\n",
    "# Load the validation dataset\n",
    "with open('val_dataset.json', 'r', encoding='utf-8') as file:\n",
    "    val_data = json.load(file)\n",
    "\n",
    "# Initialize a dictionary to store the validation output\n",
    "val_output = defaultdict(dict)\n",
    "\n",
    "# Process each data instance in the validation dataset\n",
    "for instance in val_data:\n",
    "    case_id = instance['id']\n",
    "    text = instance['data']['text']\n",
    "    annotations = instance['annotations']\n",
    "    bio_labels = assign_bio_labels(text, annotations)\n",
    "\n",
    "    # Store the text and BIO labels in the validation output dictionary\n",
    "    val_output[case_id]['text'] = text\n",
    "    val_output[case_id]['labels'] = bio_labels\n",
    "\n",
    "# Save the validation output to a JSON file\n",
    "with open('val_bio_labels.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(val_output, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "############################################################\n",
    "# Load the test dataset\n",
    "with open('NER_TEST_JUDGEMENT.json', 'r', encoding='utf-8') as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "# Initialize a dictionary to store the test output\n",
    "test_output = defaultdict(dict)\n",
    "\n",
    "# Process each data instance in the test dataset\n",
    "for instance in test_data:\n",
    "    case_id = instance['id']\n",
    "    text = instance['data']['text']\n",
    "    annotations = instance['annotations']\n",
    "    bio_labels = assign_bio_labels(text, annotations)\n",
    "\n",
    "    # Store the text and BIO labels in the test output dictionary\n",
    "    test_output[case_id]['text'] = text\n",
    "    test_output[case_id]['labels'] = bio_labels\n",
    "\n",
    "# Save the test output to a JSON file\n",
    "with open('test_bio_labels.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(test_output, file, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
