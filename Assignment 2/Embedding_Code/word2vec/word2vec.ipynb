{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_data_sample': {'03f3901e95ed493b866bd7807f623bc0': {'text': \"True, our Constitution has no 'due process' clause or the VIII Amendment; but, in this branch of law, after R.C. Cooper v. Union of India, (1970) 1 SCC 248 and Maneka Gandhi v. Union of India, (1978) 1 SCC 248, the consequence is the same.\",\n",
       "   'labels': ['O',\n",
       "    'O',\n",
       "    'B_STATUTE',\n",
       "    'I_STATUTE',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'B_PRECEDENT',\n",
       "    'I_PRECEDENT',\n",
       "    'I_PRECEDENT',\n",
       "    'I_PRECEDENT',\n",
       "    'I_PRECEDENT',\n",
       "    'I_PRECEDENT',\n",
       "    'I_PRECEDENT',\n",
       "    'I_PRECEDENT',\n",
       "    'I_PRECEDENT',\n",
       "    'I_PRECEDENT',\n",
       "    'I_PRECEDENT',\n",
       "    'B_PRECEDENT',\n",
       "    'I_PRECEDENT',\n",
       "    'I_PRECEDENT',\n",
       "    'I_PRECEDENT',\n",
       "    'I_PRECEDENT',\n",
       "    'I_PRECEDENT',\n",
       "    'I_PRECEDENT',\n",
       "    'I_PRECEDENT',\n",
       "    'I_PRECEDENT',\n",
       "    'I_PRECEDENT',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O']},\n",
       "  'b0311cba3aac4d909eec6e156c059617': {'text': '(See Principles of Statutory Interpretation by Justice G.P. Singh, 9th Edn., 2004 at p. \\n\\n 438.).',\n",
       "   'labels': ['O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'B_JUDGE',\n",
       "    'I_JUDGE',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O']}},\n",
       " 'train_data_sample': {'b29019e16fc64e5da48f20706b152fae': {'text': 'Therefore, while interpreting statutory provisions, the courts should keep in mind the objectives or purpose for which statute has been enacted.',\n",
       "   'labels': ['O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O']},\n",
       "  'b8f6ac625fb9435db21c94644bf7784f': {'text': 'The petitioner in W.P.No.15821 of 2008 was never considered for appointment under the National Rural Employment Guarantee Scheme either through Employment Exchange sponsorship or by Outsourcing Agencies.',\n",
       "   'labels': ['O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'B_CASE_NUMBER',\n",
       "    'I_CASE_NUMBER',\n",
       "    'I_CASE_NUMBER',\n",
       "    'I_CASE_NUMBER',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'B_ORG',\n",
       "    'I_ORG',\n",
       "    'I_ORG',\n",
       "    'I_ORG',\n",
       "    'I_ORG',\n",
       "    'I_ORG',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O']}},\n",
       " 'val_data_sample': {'35a7bbe300734dffae0740ab00356e1d': {'text': 'Clause 18(1), (2) and (3)\\n(a) & (b) were transposed in Article 23 of the Draft Constitution of India.',\n",
       "   'labels': ['B_PROVISION',\n",
       "    'I_PROVISION',\n",
       "    'I_PROVISION',\n",
       "    'I_PROVISION',\n",
       "    'I_PROVISION',\n",
       "    'I_PROVISION',\n",
       "    'I_PROVISION',\n",
       "    'I_PROVISION',\n",
       "    'O',\n",
       "    'O',\n",
       "    'B_PROVISION',\n",
       "    'I_PROVISION',\n",
       "    'I_PROVISION',\n",
       "    'O',\n",
       "    'B_STATUTE',\n",
       "    'I_STATUTE',\n",
       "    'I_STATUTE',\n",
       "    'I_STATUTE']},\n",
       "  'b6d5aee6565043c799ece82bc2b05e43': {'text': 'The order cannot be said to be wrong when the only ground mentioned for impleading the Chief Minister as a party was to make it incumbent on him to file an affidavit, which he was not legally obliged to, if he was not a party.',\n",
       "   'labels': ['O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O',\n",
       "    'O']}}}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define a function to load data from a file\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# Load the data from the three files\n",
    "test_data_path = 'test_bio_labels.json'\n",
    "train_data_path = 'train_bio_labels.json'\n",
    "val_data_path = 'val_bio_labels.json'\n",
    "\n",
    "test_data = load_data(test_data_path)\n",
    "train_data = load_data(train_data_path)\n",
    "val_data = load_data(val_data_path)\n",
    "# Adjusting approach to properly display the data structure and a sample\n",
    "\n",
    "# Function to safely extract samples from data which might be a dictionary or list\n",
    "def get_sample(data):\n",
    "    if isinstance(data, dict):\n",
    "        # If the data is a dictionary, return a part of it\n",
    "        return {k: data[k] for k in list(data)[:2]}\n",
    "    elif isinstance(data, list):\n",
    "        # If the data is a list, return the first two elements\n",
    "        return data[:2]\n",
    "    else:\n",
    "        return \"Unsupported data type\"\n",
    "\n",
    "# Get samples from each dataset\n",
    "test_data_sample = get_sample(test_data)\n",
    "train_data_sample = get_sample(train_data)\n",
    "val_data_sample = get_sample(val_data)\n",
    "\n",
    "{\n",
    "    \"test_data_sample\": test_data_sample,\n",
    "    \"train_data_sample\": train_data_sample,\n",
    "    \"val_data_sample\": val_data_sample,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Therefore, while interpreting statutory provis...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The petitioner in W.P.No.15821 of 2008 was nev...</td>\n",
       "      <td>[O, O, O, B_CASE_NUMBER, I_CASE_NUMBER, I_CASE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The factum of accident, allegation of rash and...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, B_OTHE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>..36.. \\n\\n W.A.No.655/2012 &amp; others Meaning t...</td>\n",
       "      <td>[O, O, B_CASE_NUMBER, I_CASE_NUMBER, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The law on this issue is well settled and the ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Therefore, while interpreting statutory provis...   \n",
       "1  The petitioner in W.P.No.15821 of 2008 was nev...   \n",
       "2  The factum of accident, allegation of rash and...   \n",
       "3  ..36.. \\n\\n W.A.No.655/2012 & others Meaning t...   \n",
       "4  The law on this issue is well settled and the ...   \n",
       "\n",
       "                                              labels  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1  [O, O, O, B_CASE_NUMBER, I_CASE_NUMBER, I_CASE...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, B_OTHE...  \n",
       "3  [O, O, B_CASE_NUMBER, I_CASE_NUMBER, O, O, O, ...  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "\n",
    "# Convert dictionary data to DataFrame\n",
    "train_df = pd.DataFrame.from_dict(train_data, orient='index').reset_index(drop=True)\n",
    "test_df = pd.DataFrame.from_dict(test_data, orient='index').reset_index(drop=True)\n",
    "val_df = pd.DataFrame.from_dict(val_data, orient='index').reset_index(drop=True)\n",
    "\n",
    "# Display a sample from the train DataFrame to check preprocessing\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the dataframe to csv\n",
    "train_df.to_csv('train_df.csv', index=False)\n",
    "test_df.to_csv('test_df.csv', index=False)\n",
    "val_df.to_csv('val_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Prepare sentences for Word2Vec training\n",
    "train_sentences = train_df['text'].apply(lambda x: x.split()).tolist()\n",
    "test_sentences = test_df['text'].apply(lambda x: x.split()).tolist()\n",
    "val_sentences = val_df['text'].apply(lambda x: x.split()).tolist()\n",
    "\n",
    "# Combine sentences from all datasets\n",
    "all_sentences = train_sentences + test_sentences + val_sentences\n",
    "\n",
    "# Train the Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=all_sentences, vector_size=512, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Save the model\n",
    "word2vec_model.save('word2vec_model')\n",
    "\n",
    "#example\n",
    "examples_sentence = train_sentences[0]\n",
    "\n",
    "# Get the word vectors for the example sentence\n",
    "example_vectors = [word2vec_model.wv[word] for word in examples_sentence if word in word2vec_model.wv.key_to_index]\n",
    "\n",
    "# Now similarly, convert all the sentences in train_df to vectors\n",
    "train_df['vectors'] = train_df['text'].apply(lambda x: [word2vec_model.wv[word] for word in x.split() if word in word2vec_model.wv.key_to_index])\n",
    "val_df['vectors'] = val_df['text'].apply(lambda x: [word2vec_model.wv[word] for word in x.split() if word in word2vec_model.wv.key_to_index])\n",
    "test_df['vectors'] = test_df['text'].apply(lambda x: [word2vec_model.wv[word] for word in x.split() if word in word2vec_model.wv.key_to_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "#print the length of text of first instance in train_df\n",
    "print(len(train_df['text'][2].split()))\n",
    "print(len(train_df['vectors'][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if the length of the vectors is the same as the length of the text for all instances\n",
    "train_df['len_text'] = train_df['text'].apply(lambda x: len(x.split()))\n",
    "train_df['len_vectors'] = train_df['vectors'].apply(lambda x: len(x))\n",
    "train_df['len_text'].equals(train_df['len_vectors'])\n",
    "\n",
    "#remove the columns\n",
    "train_df = train_df.drop(columns=['len_text', 'len_vectors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the vectors to have the same length\n",
    "def pad_vectors(vectors, target_length=70):\n",
    "    # Get the current length of the vectors\n",
    "    current_length = len(vectors)\n",
    "    \n",
    "    # If the current length is less than the target length, pad the vectors\n",
    "    if current_length < target_length:\n",
    "        # Calculate the number of vectors to be added\n",
    "        num_to_add = target_length - current_length\n",
    "        \n",
    "        # Add the vectors\n",
    "        vectors += [np.zeros(512) for _ in range(num_to_add)]\n",
    "    \n",
    "    # if the current length is greater than the target length, slice the vectors\n",
    "    elif current_length > target_length:\n",
    "        vectors = vectors[:target_length]\n",
    "\n",
    "    return vectors\n",
    "\n",
    "# Pad the vectors in the train, test, and val DataFrames\n",
    "train_df['vectors'] = train_df['vectors'].apply(lambda x: pad_vectors(x, target_length=70))\n",
    "val_df['vectors'] = val_df['vectors'].apply(lambda x: pad_vectors(x, target_length=70))\n",
    "test_df['vectors'] = test_df['vectors'].apply(lambda x: pad_vectors(x, target_length=70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the unique labels\n",
    "unique_labels = set()\n",
    "for labels in train_df['labels']:\n",
    "    unique_labels.update(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I_RESPONDENT', 'I_PROVISION', 'B_WITNESS', 'B_CASE_NUMBER', 'B_JUDGE', 'B_PRECEDENT', 'I_PETITIONER', 'I_OTHER_PERSON', 'B_DATE', 'B_GPE', 'I_ORG', 'B_RESPONDENT', 'B_OTHER_PERSON', 'O', 'I_WITNESS', 'B_PROVISION', 'I_PRECEDENT', 'B_STATUTE', 'B_ORG', 'B_PETITIONER', 'I_DATE', 'I_STATUTE', 'I_COURT', 'I_CASE_NUMBER', 'B_COURT', 'I_GPE', 'I_JUDGE'}\n"
     ]
    }
   ],
   "source": [
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I_RESPONDENT': 0, 'I_PROVISION': 1, 'B_WITNESS': 2, 'B_CASE_NUMBER': 3, 'B_JUDGE': 4, 'B_PRECEDENT': 5, 'I_PETITIONER': 6, 'I_OTHER_PERSON': 7, 'B_DATE': 8, 'B_GPE': 9, 'I_ORG': 10, 'B_RESPONDENT': 11, 'B_OTHER_PERSON': 12, 'O': 13, 'I_WITNESS': 14, 'B_PROVISION': 15, 'I_PRECEDENT': 16, 'B_STATUTE': 17, 'B_ORG': 18, 'B_PETITIONER': 19, 'I_DATE': 20, 'I_STATUTE': 21, 'I_COURT': 22, 'I_CASE_NUMBER': 23, 'B_COURT': 24, 'I_GPE': 25, 'I_JUDGE': 26}\n"
     ]
    }
   ],
   "source": [
    "#giving all labels index\n",
    "labels_to_index = {label: index for index, label in enumerate(unique_labels)}\n",
    "print(labels_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdding the labels as well\n",
    "def pad_labels(labels, target_length=70):\n",
    "    # Get the current length of the labels\n",
    "    current_length = len(labels)\n",
    "    \n",
    "    # If the current length is less than the target length, pad the labels\n",
    "    if current_length < target_length:\n",
    "        # Calculate the number of labels to be added\n",
    "        num_to_add = target_length - current_length\n",
    "        \n",
    "        # Add the labels\n",
    "        labels += ['O' for _ in range(num_to_add)]\n",
    "    \n",
    "    # if the current length is greater than the target length, slice the labels\n",
    "    elif current_length > target_length:\n",
    "        labels = labels[:target_length]\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'dtensor' from 'tensorflow.compat.v2.experimental' (c:\\Users\\amilb\\anaconda3\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\experimental\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [96], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# need to write a simple vanilla rnn model to predict the labels\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, models\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiLabelBinarizer\n",
      "File \u001b[1;32mc:\\Users\\amilb\\anaconda3\\lib\\site-packages\\keras\\api\\_v2\\keras\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n",
      "File \u001b[1;32mc:\\Users\\amilb\\anaconda3\\lib\\site-packages\\keras\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n",
      "File \u001b[1;32mc:\\Users\\amilb\\anaconda3\\lib\\site-packages\\keras\\__internal__\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m losses\n",
      "File \u001b[1;32mc:\\Users\\amilb\\anaconda3\\lib\\site-packages\\keras\\__internal__\\backend\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _initialize_variables \u001b[38;5;28;01mas\u001b[39;00m initialize_variables\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m track_variable\n",
      "File \u001b[1;32mc:\\Users\\amilb\\anaconda3\\lib\\site-packages\\keras\\src\\__init__.py:21\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n",
      "File \u001b[1;32mc:\\Users\\amilb\\anaconda3\\lib\\site-packages\\keras\\src\\applications\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras Applications are premade architectures with pre-trained weights.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtBase\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtLarge\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtSmall\n",
      "File \u001b[1;32mc:\\Users\\amilb\\anaconda3\\lib\\site-packages\\keras\\src\\applications\\convnext.py:28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initializers\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n",
      "File \u001b[1;32mc:\\Users\\amilb\\anaconda3\\lib\\site-packages\\keras\\src\\backend.py:34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend_config\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_coordinator_utils \u001b[38;5;28;01mas\u001b[39;00m dc\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtensor_api \u001b[38;5;28;01mas\u001b[39;00m dtensor\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_tensor\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_flow_util\n",
      "File \u001b[1;32mc:\\Users\\amilb\\anaconda3\\lib\\site-packages\\keras\\src\\dtensor\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras' DTensor library.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtensor \u001b[38;5;28;01mas\u001b[39;00m dtensor_api\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'dtensor' from 'tensorflow.compat.v2.experimental' (c:\\Users\\amilb\\anaconda3\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\experimental\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# need to write a simple vanilla rnn model to predict the labels\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Convert the labels to one-hot encodings\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(train_df['labels'])\n",
    "\n",
    "# Transform the labels in the train, test, and val DataFrames\n",
    "train_labels = mlb.transform(train_df['labels'])\n",
    "val_labels = mlb.transform(val_df['labels'])\n",
    "test_labels = mlb.transform(test_df['labels'])\n",
    "\n",
    "# Define the RNN model\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(70, 512)),\n",
    "    layers.SimpleRNN(64, return_sequences=True),\n",
    "    layers.SimpleRNN(64),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(25, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['f1_score'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(np.array(train_df['vectors'].tolist()), train_labels, validation_data=(np.array(val_df['vectors'].tolist()), val_labels), epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, f1_score = model.evaluate(np.array(test_df['vectors'].tolist()), test_labels)\n",
    "\n",
    "{\n",
    "    \"loss\": loss,\n",
    "    \"f1_score\": f1_score\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
