{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loading the Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "train_path = \"/Users/arnav/Desktop/MachineLearning/CSE556-NLP-Assignments/Assignment 2/Task2_Dataset_ATE/ATE_train.json\"\n",
    "test_path = \"/Users/arnav/Desktop/MachineLearning/CSE556-NLP-Assignments/Assignment 2/Task2_Dataset_ATE/ATE_test.json\"\n",
    "val_path = \"/Users/arnav/Desktop/MachineLearning/CSE556-NLP-Assignments/Assignment 2/Task2_Dataset_ATE/ATE_val.json\"\n",
    "\n",
    "# Function to load the data from the json file\n",
    "def load_from_json(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "train_data = load_from_json(train_path)\n",
    "val_data = load_from_json(val_path)\n",
    "test_data = load_from_json(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Function to tokenise the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize text based on space\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    start = 0\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        if text[i] == ' ':\n",
    "            tokens.append(text[start:i])\n",
    "            start = i + 1\n",
    "        i += 1\n",
    "    tokens.append(text[start:i])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Function to load GLoVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe vectors from file: glove.840B.300d.txt\n",
      "Skipping word: . due to conversion error.\n",
      "Skipping word: at due to conversion error.\n",
      "Skipping word: . due to conversion error.\n",
      "Skipping word: to due to conversion error.\n",
      "Skipping word: . due to conversion error.\n",
      "Skipping word: . due to conversion error.\n",
      "Skipping word: email due to conversion error.\n",
      "Skipping word: or due to conversion error.\n",
      "Skipping word: contact due to conversion error.\n",
      "Skipping word: Email due to conversion error.\n",
      "Skipping word: on due to conversion error.\n",
      "Skipping word: At due to conversion error.\n",
      "Skipping word: by due to conversion error.\n",
      "Skipping word: in due to conversion error.\n",
      "Skipping word: emailing due to conversion error.\n",
      "Skipping word: Contact due to conversion error.\n",
      "Skipping word: at due to conversion error.\n",
      "Skipping word: â€¢ due to conversion error.\n",
      "Skipping word: at due to conversion error.\n",
      "Skipping word: is due to conversion error.\n",
      "Loaded 2195884 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "word_vectors = {}\n",
    "glove_file = \"glove.840B.300d.txt\"\n",
    "\n",
    "def load_glove_vectors(glove_file):\n",
    "    \"\"\"\n",
    "    Load GloVe vectors from a file, with error handling for non-convertible values.\n",
    "    \n",
    "    :param glove_file: Path to the GloVe file.\n",
    "    :return: Dictionary with words as keys and their embeddings as values.\n",
    "    \"\"\"\n",
    "    print(\"Loading GloVe vectors from file:\", glove_file)\n",
    "    \n",
    "    glove_vectors = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            try:\n",
    "                embedding = [float(val) for val in split_line[1:]]\n",
    "                glove_vectors[word] = embedding\n",
    "            except ValueError:\n",
    "                # Skip this word if any value is not a float\n",
    "                print(f\"Skipping word: {word} due to conversion error.\")\n",
    "                continue\n",
    "            \n",
    "    print(f\"Loaded {len(glove_vectors)} word vectors.\")\n",
    "    return glove_vectors\n",
    "\n",
    "# Call the function to load the GloVe vectors\n",
    "word_vectors = load_glove_vectors(glove_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get glove embedding for a sentence\n",
    "def get_glove_embedding(sentence):\n",
    "\n",
    "    # tokenize the sentence\n",
    "    tokens = tokenize_text(sentence)\n",
    "\n",
    "    # create a numpy array to store the embeddings\n",
    "    embeddings = np.zeros((len(tokens), 300))\n",
    "\n",
    "    # iterate through the tokens and get the embeddings\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in word_vectors:\n",
    "            embeddings[i] = word_vectors[token]\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 300)\n"
     ]
    }
   ],
   "source": [
    "# Test the function on a sentence\n",
    "sentence = train_data[\"1\"][\"text\"]\n",
    "embeddings = get_glove_embedding(sentence)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of the sentence in the training data: 78\n",
      "Maximum length of the sentence in the validation data: 83\n",
      "Maximum length of the sentence in the test data: 71\n"
     ]
    }
   ],
   "source": [
    "# Function to get the maximum length of the sentence\n",
    "def get_max_length(dict):\n",
    "    max_length = 0\n",
    "    for key, val_dict in dict.items():\n",
    "        sentence = val_dict[\"text\"]\n",
    "        tokens = tokenize_text(sentence)\n",
    "        if len(tokens) > max_length:\n",
    "            max_length = len(tokens)\n",
    "\n",
    "    return max_length\n",
    "\n",
    "# Get the maximum length of the sentence\n",
    "max_length_train = get_max_length(train_data)\n",
    "max_length_val = get_max_length(val_data)\n",
    "max_length_test = get_max_length(test_data) \n",
    "\n",
    "print(\"Maximum length of the sentence in the training data:\", max_length_train)\n",
    "print(\"Maximum length of the sentence in the validation data:\", max_length_val)\n",
    "print(\"Maximum length of the sentence in the test data:\", max_length_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the glove embeddings for the sentences\n",
    "train_embeddings = {}\n",
    "val_embeddings = {}\n",
    "test_embeddings = {}\n",
    "\n",
    "for key, dict in train_data.items():\n",
    "    sentence = dict[\"text\"]\n",
    "    embeddings = get_glove_embedding(sentence)\n",
    "    train_embeddings[key] = embeddings\n",
    "\n",
    "for key, dict in val_data.items():\n",
    "    sentence = dict[\"text\"]\n",
    "    embeddings = get_glove_embedding(sentence)\n",
    "    val_embeddings[key] = embeddings\n",
    "\n",
    "for key, dict in test_data.items():\n",
    "    sentence = dict[\"text\"]\n",
    "    embeddings = get_glove_embedding(sentence)\n",
    "    test_embeddings[key] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the training embeddings: 906\n",
      "Size of the validation embeddings: 219\n",
      "Size of the test embeddings: 328\n"
     ]
    }
   ],
   "source": [
    "# Print the size for the embeddings\n",
    "print(\"Size of the training embeddings:\", len(train_embeddings))\n",
    "print(\"Size of the validation embeddings:\", len(val_embeddings))\n",
    "print(\"Size of the test embeddings:\", len(test_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump the embeddings to 3 pickle files\n",
    "train_embeddings_file = \"Task2_GLoVe_train_embeddings.pkl\"\n",
    "val_embeddings_file = \"Task2_GLoVe_val_embeddings.pkl\"\n",
    "test_embeddings_file = \"Task2_GLoVe_test_embeddings.pkl\"\n",
    "\n",
    "with open(train_embeddings_file, \"wb\") as file:\n",
    "    pickle.dump(train_embeddings, file)\n",
    "\n",
    "with open(val_embeddings_file, \"wb\") as file:\n",
    "    pickle.dump(val_embeddings, file)\n",
    "\n",
    "with open(test_embeddings_file, \"wb\") as file:\n",
    "    pickle.dump(test_embeddings, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Load Pickle Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load these embeddings from the pickle files\n",
    "train_embeddings_file = \"Task2_GLoVe_train_embeddings.pkl\"\n",
    "val_embeddings_file = \"Task2_GLoVe_val_embeddings.pkl\"\n",
    "test_embeddings_file = \"Task2_GLoVe_test_embeddings.pkl\"\n",
    "\n",
    "train_embeddings_loaded = pickle.load(open(train_embeddings_file, \"rb\"))\n",
    "val_embeddings_loaded = pickle.load(open(val_embeddings_file, \"rb\"))\n",
    "test_embeddings_loaded = pickle.load(open(test_embeddings_file, \"rb\"))\n",
    "\n",
    "# Load the labels from the json files\n",
    "train_labels = {}\n",
    "val_labels = {}\n",
    "test_labels = {}\n",
    "\n",
    "train_label_path = \"/Users/arnav/Desktop/MachineLearning/CSE556-NLP-Assignments/Assignment 2/Task2_Dataset_ATE/ATE_train_labels.json\"\n",
    "val_label_path = \"/Users/arnav/Desktop/MachineLearning/CSE556-NLP-Assignments/Assignment 2/Task2_Dataset_ATE/ATE_val_labels.json\"\n",
    "test_label_path = \"/Users/arnav/Desktop/MachineLearning/CSE556-NLP-Assignments/Assignment 2/Task2_Dataset_ATE/ATE_test_labels.json\"\n",
    "\n",
    "train_labels = load_from_json(train_label_path)\n",
    "val_labels = load_from_json(val_label_path)\n",
    "test_labels = load_from_json(test_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each key, pad the embeddings and labels to the maximum length of 559\n",
    "# For padding labels, use 'O' tag\n",
    "# For padding embeddings, use a vector of zeros\n",
    "max_length = 83\n",
    "\n",
    "for key in train_embeddings_loaded:\n",
    "    label = train_labels[key]\n",
    "    embeddings = train_embeddings_loaded[key]\n",
    "\n",
    "    # Pad the labels\n",
    "    if len(label) < max_length:\n",
    "        label = label + ['O'] * (max_length - len(label))\n",
    "\n",
    "    # Pad the embeddings\n",
    "    if len(embeddings) < max_length:\n",
    "        embeddings = np.concatenate((embeddings, np.zeros((max_length - len(embeddings), 300))), axis=0)\n",
    "\n",
    "    train_labels[key] = label\n",
    "    train_embeddings_loaded[key] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219'])\n"
     ]
    }
   ],
   "source": [
    "print(val_embeddings_loaded.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in val_embeddings_loaded:\n",
    "    label = val_labels[key]\n",
    "    embeddings = val_embeddings_loaded[key]\n",
    "\n",
    "    # Pad the labels\n",
    "    if len(label) < max_length:\n",
    "        label = label + ['O'] * (max_length - len(label))\n",
    "\n",
    "    # Pad the embeddings\n",
    "    if len(embeddings) < max_length:\n",
    "        embeddings = np.concatenate((embeddings, np.zeros((max_length - len(embeddings), 300))), axis=0)\n",
    "\n",
    "    val_labels[key] = label\n",
    "    val_embeddings_loaded[key] = embeddings\n",
    "\n",
    "for key in test_embeddings_loaded:\n",
    "    label = test_labels[key]\n",
    "    embeddings = test_embeddings_loaded[key]\n",
    "\n",
    "    # Pad the labels\n",
    "    if len(label) < max_length:\n",
    "        label = label + ['O'] * (max_length - len(label))\n",
    "\n",
    "    # Pad the embeddings\n",
    "    if len(embeddings) < max_length:\n",
    "        embeddings = np.concatenate((embeddings, np.zeros((max_length - len(embeddings), 300))), axis=0)\n",
    "\n",
    "    test_labels[key] = label\n",
    "    test_embeddings_loaded[key] = embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
