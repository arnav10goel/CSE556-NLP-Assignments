{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "with open('NER_TRAIN_JUDGEMENT.json', 'r', encoding='utf-8') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# print('Number of sentences in the dataset:', len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets with an 85:15 ratio (randomly stratified)\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the split datasets into separate JSON files\n",
    "with open('train_dataset.json', 'w', encoding='utf-8') as train_file:\n",
    "    json.dump(train_data, train_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open('val_dataset.json', 'w', encoding='utf-8') as val_file:\n",
    "    json.dump(val_data, val_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# print(\"Training set size:\", len(train_data))\n",
    "# print(\"Validation set size:\", len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "#Assign BIO labels\n",
    "def assign_bio_labels(text, annotations):\n",
    "    #Initialize a list to store tokenized text\n",
    "    tokens=[]\n",
    "    start=0\n",
    "    i=0\n",
    "    flag=0\n",
    "\n",
    "    #Tokenize the text based on space using two pointers\n",
    "    while i<len(text):\n",
    "        if text[i]==' ':\n",
    "            tokens.append([start, text[start:i], i-1])\n",
    "            start=i+1\n",
    "        i+=1\n",
    "    tokens.append([start,text[start:i],i-1])  # [Specified format[0, 'The', 2], [4, 'quick', 8],......]\n",
    "    # print(tokens)\n",
    "\n",
    "    #BIO labels for each token stored in a different list\n",
    "    bio_labels=['O']*len(tokens)\n",
    "\n",
    "    #Updating the BIO labels from 'O' to 'B' or 'I' based on the index annotations\n",
    "    for annotation in annotations:\n",
    "        for result in annotation['result']:\n",
    "            start=result['value']['start']\n",
    "            end=result['value']['end']\n",
    "            label=result['value']['labels'][0]\n",
    "            num_iter=1\n",
    "            for i,token in enumerate(tokens):\n",
    "                # print(num)\n",
    "                if (start-1==token[0] or start==token[0])  and token[2]<=end+1:\n",
    "                    if num_iter!=1:\n",
    "                        flag=1\n",
    "                        # print(\"1\",token[1])\n",
    "                        break\n",
    "                    bio_labels[i]='B_' +label\n",
    "                    num_iter+=1\n",
    "                elif start-1<token[0]<=end+1:\n",
    "                    if num_iter<2:\n",
    "                        flag=1\n",
    "                        # print(\"2\",token[1])\n",
    "                        break\n",
    "                    bio_labels[i]='I_'+label\n",
    "                    num_iter+=1\n",
    "            if flag==1:\n",
    "                break\n",
    "    return bio_labels,flag\n",
    "\n",
    "############################################################\n",
    "# Load the train dataset\n",
    "with open('train_dataset.json', 'r', encoding='utf-8') as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "# Initialize a dictionary to store the train output\n",
    "train_output = defaultdict(dict)\n",
    "\n",
    "# Process each data instance in the train dataset\n",
    "for instance in train_data:\n",
    "    case_id = instance['id']\n",
    "    text = instance['data']['text']\n",
    "    annotations = instance['annotations']\n",
    "    bio_labels, flag = assign_bio_labels(text, annotations)\n",
    "    if flag==0:\n",
    "        # Store the text and BIO labels in the train output dictionary\n",
    "        train_output[case_id]['text'] = text\n",
    "        train_output[case_id]['labels'] = bio_labels\n",
    "\n",
    "# Save the train output to a JSON file\n",
    "with open('train_bio_labels.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(train_output, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "############################################################\n",
    "# Load the validation dataset\n",
    "with open('val_dataset.json', 'r', encoding='utf-8') as file:\n",
    "    val_data = json.load(file)\n",
    "\n",
    "# Initialize a dictionary to store the validation output\n",
    "val_output = defaultdict(dict)\n",
    "\n",
    "# Process each data instance in the validation dataset\n",
    "for instance in val_data:\n",
    "    case_id = instance['id']\n",
    "    text = instance['data']['text']\n",
    "    annotations = instance['annotations']\n",
    "    bio_labels, flag = assign_bio_labels(text, annotations)\n",
    "\n",
    "    if flag==0:\n",
    "        # Store the text and BIO labels in the validation output dictionary\n",
    "        val_output[case_id]['text'] = text\n",
    "        val_output[case_id]['labels'] = bio_labels\n",
    "\n",
    "# Save the validation output to a JSON file\n",
    "with open('val_bio_labels.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(val_output, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "############################################################\n",
    "# Load the test dataset\n",
    "with open('NER_TEST_JUDGEMENT.json', 'r', encoding='utf-8') as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "# Initialize a dictionary to store the test output\n",
    "test_output = defaultdict(dict)\n",
    "\n",
    "# Process each data instance in the test dataset\n",
    "for instance in test_data:\n",
    "    case_id = instance['id']\n",
    "    text = instance['data']['text']\n",
    "    annotations = instance['annotations']\n",
    "    bio_labels, flag = assign_bio_labels(text, annotations)\n",
    "\n",
    "    if flag==0:\n",
    "        # Store the text and BIO labels in the test output dictionary\n",
    "        test_output[case_id]['text'] = text\n",
    "        test_output[case_id]['labels'] = bio_labels\n",
    "\n",
    "# Save the test output to a JSON file\n",
    "with open('test_bio_labels.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(test_output, file, ensure_ascii=False, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
