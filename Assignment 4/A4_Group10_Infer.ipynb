{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "- Loading validation embeddings for task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embeddings from the validation set\n",
    "val_embeddings_file = \"val_embeddings.pkl\"\n",
    "val_embeddings_loaded = pickle.load(open(val_embeddings_file, \"rb\"))\n",
    "\n",
    "# Load the labels from the validation set\n",
    "task1_val_labels = \"task1_labels_dev.pkl\"\n",
    "task1_val_labels_loaded = pickle.load(open(task1_val_labels, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels by a fixed mapping\n",
    "mapping = {\n",
    "    \"-1\": 0,\n",
    "    \"sadness\": 1,\n",
    "    \"joy\": 2,\n",
    "    \"fear\": 3,\n",
    "    \"anger\": 4,\n",
    "    \"surprise\": 5,\n",
    "    \"disgust\": 6,\n",
    "    \"neutral\": 7\n",
    "}\n",
    "\n",
    "# Convert the labels to integers\n",
    "val_labels = [np.array([mapping[str(label)] for label in task1_val_labels_loaded[key]]) for key in task1_val_labels_loaded.keys()]\n",
    "\n",
    "# Convert the embeddings to a list\n",
    "val_embeddings = [val_embeddings_loaded[key] for key in val_embeddings_loaded.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class and Dataloader for Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings  # List of embedding matrices\n",
    "        self.labels = labels  # List of label arrays\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.embeddings[idx]), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    embeddings, labels = zip(*batch)\n",
    "    embeddings_pad = torch.nn.utils.rnn.pad_sequence(embeddings, batch_first=True, padding_value=0)\n",
    "    labels_pad = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-1)  # Use -1 for padding\n",
    "    return embeddings_pad, labels_pad\n",
    "\n",
    "# Make val dataset and dataloader\n",
    "val_dataset = EmotionDataset(val_embeddings, val_labels)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - M1 (Task 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "# Initialize the model\n",
    "INPUT_SIZE = 384 # Dimension of the input embeddings\n",
    "HIDDEN_SIZE = 128 # Dimension of the hidden state\n",
    "OUTPUT_SIZE = 8 # Number of classes\n",
    "\n",
    "MODEL_M1 = RNNModel(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "# Load the model from the saved state\n",
    "TASK_1_MODEL_PATH = \"M1_Task1.pth\"\n",
    "MODEL_M1.load_state_dict(torch.load(TASK_1_MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute F1 metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predictions and labels should be flattened arrays\n",
    "def compute_metrics(predictions, labels):\n",
    "    mask = labels != -1  # Ignore padded labels\n",
    "    masked_labels = labels[mask]\n",
    "    masked_predictions = predictions[mask]\n",
    "    weighted_f1 = f1_score(masked_labels, masked_predictions, average='weighted')\n",
    "    macro_f1 = f1_score(masked_labels, masked_predictions, average='macro')\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(masked_labels, masked_predictions)\n",
    "    return weighted_f1, macro_f1, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Weighted F1: 0.8887216999599405\n",
      "Validation Macro F1: 0.8618793949235899\n",
      "Validation Accuracy: 0.8896750902527076\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "MODEL_M1.eval()\n",
    "val_predictions = []\n",
    "val_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for embeddings, labels in val_dataloader:\n",
    "        output = MODEL_M1(embeddings)\n",
    "        \n",
    "        # Flatten the output and labels\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        # Get the predictions\n",
    "        predictions = output.argmax(dim=-1)\n",
    "\n",
    "        val_predictions.append(predictions.detach().cpu().numpy())\n",
    "        val_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "val_predictions = np.concatenate(val_predictions)\n",
    "val_labels = np.concatenate(val_labels)\n",
    "\n",
    "weighted_f1_val, macro_f1_val, acc = compute_metrics(val_predictions, val_labels)\n",
    "\n",
    "# Print the F1 scores\n",
    "print(\"Validation Weighted F1:\", weighted_f1_val)\n",
    "print(\"Validation Macro F1:\", macro_f1_val)\n",
    "print(\"Validation Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - M2\n",
    "- This is bidirectional Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.gru(x)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "# Initialize the model\n",
    "INPUT_SIZE = 384 # Dimension of the input embeddings\n",
    "HIDDEN_SIZE = 128 # Dimension of the hidden state\n",
    "OUTPUT_SIZE = 8 # Number of classes\n",
    "\n",
    "MODEL_M2 = GRUModel(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "# Load the model from the saved state\n",
    "TASK_2_MODEL_PATH = \"M2_Task1.pth\"\n",
    "MODEL_M2.load_state_dict(torch.load(TASK_2_MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Weighted F1: 0.9526625915951877\n",
      "Validation Macro F1: 0.9331200122847985\n",
      "Validation Accuracy: 0.9527797833935018\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "MODEL_M2.eval()\n",
    "\n",
    "val_predictions = []\n",
    "val_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for embeddings, labels in val_dataloader:\n",
    "        output = MODEL_M2(embeddings)\n",
    "        \n",
    "        # Flatten the output and labels\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        # Get the predictions\n",
    "        predictions = output.argmax(dim=-1)\n",
    "\n",
    "        val_predictions.append(predictions.detach().cpu().numpy())\n",
    "        val_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "val_predictions = np.concatenate(val_predictions)\n",
    "val_labels = np.concatenate(val_labels)\n",
    "\n",
    "weighted_f1_val, macro_f1_val, acc = compute_metrics(val_predictions, val_labels)\n",
    "\n",
    "# Print the F1 scores\n",
    "print(\"Validation Weighted F1:\", weighted_f1_val)\n",
    "print(\"Validation Macro F1:\", macro_f1_val)\n",
    "print(\"Validation Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "- Emotion Flip Reasoning\n",
    "- Loading validation labels for task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0, 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Load the embeddings\n",
    "val_embeddings_file = \"val_embeddings.pkl\"\n",
    "val_embeddings_loaded = pickle.load(open(val_embeddings_file, \"rb\"))\n",
    "\n",
    "# Load the labels for Task 2\n",
    "task2_val_labels = \"task2_labels_dev.pkl\"\n",
    "task2_val_labels_loaded = pickle.load(open(task2_val_labels, \"rb\"))\n",
    "\n",
    "# Get all unique labels and handle null values\n",
    "unique_labels = set()\n",
    "\n",
    "for key in task2_val_labels_loaded.keys():\n",
    "    # Check for None values and change them to 0\n",
    "    for i in range(len(task2_val_labels_loaded[key])):\n",
    "        if task2_val_labels_loaded[key][i] == None:\n",
    "            task2_val_labels_loaded[key][i] = 0\n",
    "    unique_labels.update(task2_val_labels_loaded[key])\n",
    "\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert embeddings to lists\n",
    "val_embeddings = [val_embeddings_loaded[key] for key in val_embeddings_loaded.keys()]\n",
    "\n",
    "# convert labels to lists\n",
    "val_labels = [task2_val_labels_loaded[key] for key in task2_val_labels_loaded.keys()]\n",
    "\n",
    "# Convert labels to integers\n",
    "val_labels = [[int(label) for label in labels] for labels in val_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dataset Class and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings  # List of embedding matrices\n",
    "        self.labels = labels  # List of label arrays\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.embeddings[idx]), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    embeddings, labels = zip(*batch)\n",
    "    embeddings_pad = torch.nn.utils.rnn.pad_sequence(embeddings, batch_first=True, padding_value=0)\n",
    "    labels_pad = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-1)  # Use -1 for padding\n",
    "    return embeddings_pad, labels_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make val dataset and dataloader\n",
    "val_dataset = EmotionDataset(val_embeddings, val_labels)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - M3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "# Initialize the model\n",
    "INPUT_SIZE = 384 # Dimension of the input embeddings\n",
    "HIDDEN_SIZE = 128 # Dimension of the hidden state\n",
    "OUTPUT_SIZE = 2 # Number of classes\n",
    "\n",
    "MODEL_M3 = RNNModel(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "# Load the model from the saved state\n",
    "TASK_3_MODEL_PATH = \"M3_Task2.pth\"\n",
    "MODEL_M3.load_state_dict(torch.load(TASK_3_MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute F1 metrics for Task 2\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Predictions and labels should be flattened arrays\n",
    "def compute_metrics_task2(predictions, labels):\n",
    "    # Ignore the padding value -1 \n",
    "    # Also ignore where both the prediction and label are 0\n",
    "    mask = (labels != -1) & ((labels != 0) | (predictions != 0))\n",
    "    masked_labels = labels[mask]\n",
    "    masked_predictions = predictions[mask]\n",
    "\n",
    "    if len(masked_labels) == 0 or len(masked_predictions) == 0:\n",
    "        #print(\"Warning: No valid data points were found after masking. Returning zero F1 scores.\")\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    weighted_f1 = f1_score(masked_labels, masked_predictions, average='weighted')\n",
    "    macro_f1 = f1_score(masked_labels, masked_predictions, average='macro')\n",
    "    f1 = f1_score(masked_labels, masked_predictions)\n",
    "    accuracy = accuracy_score(masked_labels, masked_predictions)\n",
    "    return weighted_f1, macro_f1, f1, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Weighted F1: 0.21824660103141114\n",
      "Validation Macro F1: 0.12222222222222222\n",
      "Validation F1: 0.24444444444444444\n",
      "Validation Accuracy: 0.13924050632911392\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "MODEL_M3.eval()\n",
    "\n",
    "val_predictions = []\n",
    "val_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for embeddings, labels in val_dataloader:\n",
    "        output = MODEL_M3(embeddings)\n",
    "        \n",
    "        # Flatten the output and labels\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        # Get the predictions\n",
    "        predictions = output.argmax(dim=-1)\n",
    "\n",
    "        val_predictions.append(predictions.detach().cpu().numpy())\n",
    "        val_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "val_predictions = np.concatenate(val_predictions)\n",
    "val_labels = np.concatenate(val_labels)\n",
    "\n",
    "weighted_f1_val, macro_f1_val, f1_val, acc = compute_metrics_task2(val_predictions, val_labels)\n",
    "\n",
    "# Print the F1 scores\n",
    "print(\"Validation Weighted F1:\", weighted_f1_val)\n",
    "print(\"Validation Macro F1:\", macro_f1_val)\n",
    "print(\"Validation F1:\", f1_val)\n",
    "print(\"Validation Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - M4\n",
    "- Bi-directional GRU model for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "# Initialize the model\n",
    "INPUT_SIZE = 384 # Dimension of the input embeddings\n",
    "HIDDEN_SIZE = 128 # Dimension of the hidden state\n",
    "OUTPUT_SIZE = 2 # Number of classes\n",
    "\n",
    "MODEL_M4 = GRUModel(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "# Load the model from the saved state\n",
    "TASK_4_MODEL_PATH = \"M4_Task2.pth\"\n",
    "MODEL_M4.load_state_dict(torch.load(TASK_4_MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Weighted F1: 0.6308483224132813\n",
      "Validation Macro F1: 0.36282722513089005\n",
      "Validation F1: 0.7256544502617801\n",
      "Validation Accuracy: 0.5694330320460148\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "MODEL_M4.eval()\n",
    "\n",
    "val_predictions = []\n",
    "val_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for embeddings, labels in val_dataloader:\n",
    "        output = MODEL_M4(embeddings)\n",
    "        \n",
    "        # Flatten the output and labels\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        # Get the predictions\n",
    "        predictions = output.argmax(dim=-1)\n",
    "\n",
    "        val_predictions.append(predictions.detach().cpu().numpy())\n",
    "        val_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "val_predictions = np.concatenate(val_predictions)\n",
    "val_labels = np.concatenate(val_labels)\n",
    "\n",
    "weighted_f1_val, macro_f1_val, f1_val, acc = compute_metrics_task2(val_predictions, val_labels)\n",
    "\n",
    "# Print the F1 scores\n",
    "print(\"Validation Weighted F1:\", weighted_f1_val)\n",
    "print(\"Validation Macro F1:\", macro_f1_val)\n",
    "print(\"Validation F1:\", f1_val)\n",
    "print(\"Validation Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model M1 on a Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I am assuming the test set is in the same format as train and validation json files\n",
    "- I will load the json file here and compute the sentence-bert embeddings\n",
    "- Will then follow same methodology as above for computing F1 score over that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "\n",
    "\"\"\"\n",
    "Load the SentenceTransformer model for encoding the sentences\n",
    "\"\"\"\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### UNCOMMENT THESE TEST CODES TO PROCEED ON TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Import the test json file\n",
    "# - Enter file path here\n",
    "# \"\"\"\n",
    "# TEST_PATH = \"val.json\"  ### ENTER HERE\n",
    "# with open(TEST_PATH) as f:\n",
    "#     test_data = json.load(f)\n",
    "\n",
    "# # Store the embeddings in a dictionary\n",
    "# test_embeddings_dict = {}\n",
    "# test_labels_task1_dict = {}\n",
    "# test_labels_task2_dict = {}\n",
    "# i = 0\n",
    "\n",
    "# for test_dict in test_data:\n",
    "#     # Get the relevant information from the dictionary\n",
    "#     episode_key = test_dict[\"episode\"]\n",
    "#     utterances = test_dict[\"utterances\"]\n",
    "\n",
    "#     # Get the emotion labels\n",
    "#     task1_labels = test_dict[\"emotions\"]\n",
    "#     task2_labels = test_dict[\"triggers\"]\n",
    "\n",
    "#     # Generate embeddings for the utterances\n",
    "#     embeddings = model.encode(utterances)\n",
    "#     i+=1\n",
    "#     # Print the episode key\n",
    "#     print(i)\n",
    "\n",
    "#     # Store the embedding\n",
    "#     test_embeddings_dict[episode_key] = embeddings\n",
    "#     test_labels_task1_dict[episode_key] = task1_labels\n",
    "#     test_labels_task2_dict[episode_key] = task2_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 - Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encode the labels by a fixed mapping\n",
    "# mapping = {\n",
    "#     \"-1\": 0,\n",
    "#     \"sadness\": 1,\n",
    "#     \"joy\": 2,\n",
    "#     \"fear\": 3,\n",
    "#     \"anger\": 4,\n",
    "#     \"surprise\": 5,\n",
    "#     \"disgust\": 6,\n",
    "#     \"neutral\": 7\n",
    "# }\n",
    "\n",
    "# # Convert the labels to integers\n",
    "# test_labels_task1 = [np.array([mapping[str(label)] for label in test_labels_task1_dict[key]]) for key in test_labels_task1_dict.keys()]\n",
    "\n",
    "# # Convert the embeddings to a list\n",
    "# test_embeddings = [test_embeddings_dict[key] for key in test_embeddings_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make test dataset and dataloader\n",
    "# test_dataset_task1 = EmotionDataset(test_embeddings, test_labels_task1)\n",
    "# test_dataloader_task1 = DataLoader(test_dataset_task1, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate Model M1 on the test set\n",
    "# MODEL_M1.eval()\n",
    "\n",
    "# test_predictions = []\n",
    "# test_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for embeddings, labels in test_dataloader_task1:\n",
    "#         output = MODEL_M1(embeddings)\n",
    "        \n",
    "#         # Flatten the output and labels\n",
    "#         output = output.view(-1, output.shape[-1])\n",
    "#         labels = labels.view(-1)\n",
    "\n",
    "#         # Get the predictions\n",
    "#         predictions = output.argmax(dim=-1)\n",
    "\n",
    "#         test_predictions.append(predictions.detach().cpu().numpy())\n",
    "#         test_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "# test_predictions = np.concatenate(test_predictions)\n",
    "# test_labels = np.concatenate(test_labels)\n",
    "\n",
    "# weighted_f1_test, macro_f1_test = compute_metrics(test_predictions, test_labels)\n",
    "\n",
    "# # Print the F1 scores\n",
    "# print(\"Model - M1 on Test Set\")\n",
    "# print(\"Test Weighted F1:\", weighted_f1_test)\n",
    "# print(\"Test Macro F1:\", macro_f1_test)\n",
    "\n",
    "# print()\n",
    "# # Evaluate Model M2 on the test set\n",
    "# MODEL_M2.eval()\n",
    "\n",
    "# test_predictions = []\n",
    "# test_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for embeddings, labels in test_dataloader_task1:\n",
    "#         output = MODEL_M2(embeddings)\n",
    "        \n",
    "#         # Flatten the output and labels\n",
    "#         output = output.view(-1, output.shape[-1])\n",
    "#         labels = labels.view(-1)\n",
    "\n",
    "#         # Get the predictions\n",
    "#         predictions = output.argmax(dim=-1)\n",
    "\n",
    "#         test_predictions.append(predictions.detach().cpu().numpy())\n",
    "#         test_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "# test_predictions = np.concatenate(test_predictions)\n",
    "# test_labels = np.concatenate(test_labels)\n",
    "\n",
    "# weighted_f1_test, macro_f1_test = compute_metrics(test_predictions, test_labels)\n",
    "\n",
    "# # Print the F1 scores\n",
    "# print(\"Model - M2 on Test Set\")\n",
    "# print(\"Test Weighted F1:\", weighted_f1_test)\n",
    "# print(\"Test Macro F1:\", macro_f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get all unique labels and handle null values\n",
    "# unique_labels = set()\n",
    "\n",
    "# for key in test_labels_task2_dict.keys():\n",
    "#     # Check for None values and change them to 0\n",
    "#     for i in range(len(test_labels_task2_dict[key])):\n",
    "#         if test_labels_task2_dict[key][i] == None:\n",
    "#             test_labels_task2_dict[key][i] = 0\n",
    "#     unique_labels.update(test_labels_task2_dict[key])\n",
    "\n",
    "# print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert embeddings to lists\n",
    "# test_embeddings = [test_embeddings_dict[key] for key in test_embeddings_dict.keys()]\n",
    "\n",
    "# # convert labels to lists\n",
    "# test_labels = [test_labels_task2_dict[key] for key in test_labels_task2_dict.keys()]\n",
    "\n",
    "# # Convert labels to integers\n",
    "# test_labels = [[int(label) for label in labels] for labels in test_labels]\n",
    "\n",
    "# # Make test dataset and dataloader\n",
    "# test_dataset_task2 = EmotionDataset(test_embeddings, test_labels)\n",
    "# test_dataloader_task2 = DataLoader(test_dataset_task2, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate Model M3 on the test set\n",
    "# MODEL_M3.eval()\n",
    "\n",
    "# test_predictions = []\n",
    "# test_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for embeddings, labels in test_dataloader_task2:\n",
    "#         output = MODEL_M3(embeddings)\n",
    "        \n",
    "#         # Flatten the output and labels\n",
    "#         output = output.view(-1, output.shape[-1])\n",
    "#         labels = labels.view(-1)\n",
    "\n",
    "#         # Get the predictions\n",
    "#         predictions = output.argmax(dim=-1)\n",
    "\n",
    "#         test_predictions.append(predictions.detach().cpu().numpy())\n",
    "#         test_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "# test_predictions = np.concatenate(test_predictions)\n",
    "# test_labels = np.concatenate(test_labels)\n",
    "\n",
    "# weighted_f1_test, macro_f1_test, f1_test = compute_metrics_task2(test_predictions, test_labels)\n",
    "\n",
    "# # Print the F1 scores\n",
    "# print(\"Model - M3 on Test Set\")\n",
    "# print(\"Test Weighted F1:\", weighted_f1_test)\n",
    "# print(\"Test Macro F1:\", macro_f1_test)\n",
    "# print(\"Test F1:\", f1_test)\n",
    "\n",
    "# print()\n",
    "# # Evaluate Model M4 on the test set\n",
    "# MODEL_M4.eval()\n",
    "\n",
    "# test_predictions = []\n",
    "# test_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for embeddings, labels in test_dataloader_task2:\n",
    "#         output = MODEL_M4(embeddings)\n",
    "        \n",
    "#         # Flatten the output and labels\n",
    "#         output = output.view(-1, output.shape[-1])\n",
    "#         labels = labels.view(-1)\n",
    "\n",
    "#         # Get the predictions\n",
    "#         predictions = output.argmax(dim=-1)\n",
    "\n",
    "#         test_predictions.append(predictions.detach().cpu().numpy())\n",
    "#         test_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "# test_predictions = np.concatenate(test_predictions)\n",
    "# test_labels = np.concatenate(test_labels)\n",
    "\n",
    "# weighted_f1_test, macro_f1_test, f1_test = compute_metrics_task2(test_predictions, test_labels)\n",
    "\n",
    "# # Print the F1 scores\n",
    "# print(\"Model - M4 on Test Set\")\n",
    "# print(\"Test Weighted F1:\", weighted_f1_test)\n",
    "# print(\"Test Macro F1:\", macro_f1_test)\n",
    "# print(\"Test F1:\", f1_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
