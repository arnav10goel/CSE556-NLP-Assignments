{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRUModel(\n",
       "  (gru): GRU(384, 128, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=256, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Specify the path to the .pth file\n",
    "model_path = \"M2_Task1.pth\"\n",
    "\n",
    "# Initialize the model\n",
    "INPUT_SIZE = 384 # Dimension of the input embeddings\n",
    "HIDDEN_SIZE = 128 # Dimension of the hidden state\n",
    "OUTPUT_SIZE = 8 # Number of classes\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.gru(x)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "# Load the saved model\n",
    "model = GRUModel(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load these embeddings from the pickle files\n",
    "val_embeddings_file = \"val_embeddings.pkl\"\n",
    "val_embeddings_loaded = pickle.load(open(val_embeddings_file, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels_file = \"task1_labels_dev.pkl\"\n",
    "val_labels_loaded = pickle.load(open(val_labels_file, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_speaker_ids_file = \"speakers_dev.pkl\"\n",
    "val_speaker_ids_loaded = pickle.load(open(val_speaker_ids_file, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to predict emotions for a given set of embeddings\n",
    "def predict_emotions(embeddings):\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for embedding in embeddings:\n",
    "            embedding_tensor = torch.tensor(embedding)\n",
    "            output = model(embedding_tensor.unsqueeze(0))\n",
    "            predicted_label = output.argmax().item()\n",
    "            predictions.append(predicted_label)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Carl', 'Both', 'Tom', 'Kim', 'Mr. Geller', 'Kate', 'Stage Manager', 'Earl', 'Mrs. Tedlock', 'Frank', 'Dr. Long', 'Megan', 'Young Ethan', 'Phoebe Sr', 'All', 'Joey', 'Elizabeth', 'Charlton Heston', 'Emily', 'Mike', 'Professore Clerk', 'Janine', 'Charlie', 'Wayne', 'The Director', 'Richard', 'an', 'Kyle', 'Joey and Chandler', 'Alice', 'A Student', 'Julio', 'Susan', 'Julie', 'Phoebe', 'Mrs. Geller', 'The Casting Director', 'Leslie', 'Dr. Drake Remoray', 'Dana', 'Stranger', 'Whitney', 'Stage Director', 'Phoebe Sr.', 'Tag', 'Jill', 'Duncan', 'Ben', 'Jester', 'Joey/Drake', 'Guy #1', 'The Woman', 'Mona', \"Joey's Hand Twin\", \"Mona's Date\", 'Kristen', 'Dr. Oberman', 'Boy in the Cape', 'Joshua', 'Ms. McKenna', 'Fireman No. 3', 'Jane', 'Rachel', 'Allesandro', 'Chloe', 'The Guys', 'Paul', 'Nurse', 'Mrs. Green', 'Mark', 'Stanley', 'Nancy', 'Mischa', 'Kristin', 'Pete', 'Mr. Heckles', 'Bobby', 'The Vendor', 'The Singing Man', 'Dina', 'Issac', 'The Museum Official', 'Roger', 'Krista', 'Message', 'Monica', 'Ross', 'Isabella', 'Hoshi', 'Doug', 'Ursula', 'The Interviewer', 'Dr. Green', 'Mr. Tribbiani', 'Stevens', 'Cassie', 'Receptionist', 'Director', 'Eric', 'Kathy', '3rd Customer', 'David', '1st Customer', 'Max', 'Gunther', 'Danny', 'Lauren', 'The Security Guard', 'Paleontologist', 'Flight Attendant', 'Mindy', 'Woman', 'The Head Librarian', 'Janice', 'Dr. Wesley', 'The Grip', 'Chip', 'The Potential Roommate', 'Lydia', 'Gary', 'Fireman No. 2', 'Russell', 'Barry', '2nd Customer', 'Casey', 'The Fireman', 'Dr. Zane', 'Bonnie', 'Carol', 'Mr. Treeger', 'Cliff', 'Guy', 'Man', 'Mr. Posner', 'Jade', 'Student', 'The Lurker', 'Lorraine', 'Tony', 'Fireman No. 1', 'Joanna', 'Katie', 'Phoebe and Rachel', 'Hitchhiker', 'Estelle', 'Larry', 'Steve', 'Aunt Lillian', 'Girl', 'Bernice', 'Teacher', 'Terry', 'The Smoking Woman', 'The Instructor', 'Chandler'}\n"
     ]
    }
   ],
   "source": [
    "# Get all unique speakers\n",
    "unique_speakers = set()\n",
    "for episode_id in val_speaker_ids_loaded:\n",
    "    unique_speakers.update(val_speaker_ids_loaded[episode_id])\n",
    "\n",
    "print(unique_speakers)\n",
    "# Create separate dictionaries for each speaker to store predicted emotions\n",
    "speaker_predictions_dict = {speaker: {} for speaker in unique_speakers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict emotions for each utterance and associate with each speaker\n",
    "for episode_id, embeddings in val_embeddings_loaded.items():\n",
    "    predictions = predict_emotions(embeddings)\n",
    "    speakers = val_speaker_ids_loaded[episode_id]\n",
    "    for speaker in speakers:\n",
    "        # Filter predictions for the current speaker\n",
    "        speaker_predictions = [pred for pred, spk in zip(predictions, val_speaker_ids_loaded[episode_id]) if spk == speaker]\n",
    "        speaker_predictions_dict[speaker][episode_id] = speaker_predictions\n",
    "\n",
    "# for speaker in speaker_predictions_dict:\n",
    "#     print(f\"Speaker: {speaker}\")\n",
    "#     for episode_id, predictions in speaker_predictions_dict[speaker].items():\n",
    "#         print(f\"Episode: {episode_id}, Predictions: {predictions}\")\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Encode the labels by a fixed mapping\n",
    "mapping = {\n",
    "    \"-1\": 0,\n",
    "    \"sadness\": 1,\n",
    "    \"joy\": 2,\n",
    "    \"fear\": 3,\n",
    "    \"anger\": 4,\n",
    "    \"surprise\": 5,\n",
    "    \"disgust\": 6,\n",
    "    \"neutral\": 7\n",
    "}\n",
    "\n",
    "# Function to detect emotion flips for a given sequence of emotions\n",
    "def detect_emotion_flips(predicted_emotions, true_emotions):\n",
    "    valid_flips = []\n",
    "    invalid_flips = []\n",
    "    for i in range(1, len(predicted_emotions)):\n",
    "        if predicted_emotions[i] != predicted_emotions[i - 1]:\n",
    "            if predicted_emotions[i-1] == true_emotions[i-1] and predicted_emotions[i] == true_emotions[i]:\n",
    "                valid_flips.append((i - 1, i))  # Store the indices of the flip\n",
    "            else:\n",
    "                invalid_flips.append((i - 1, i))\n",
    "            \n",
    "    return valid_flips, invalid_flips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totalnum_flips(true_emotions):\n",
    "    total_flips = 0\n",
    "    for i in range(1, len(true_emotions)):\n",
    "        if true_emotions[i] != true_emotions[i - 1]:\n",
    "            total_flips += 1\n",
    "    return total_flips\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.2909441233140655\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate emotion flips and metrics for each speaker\n",
    "speaker_metrics = {}\n",
    "final=0\n",
    "final_correct=0\n",
    "\n",
    "for speaker in unique_speakers:\n",
    "    valid_flips = []\n",
    "    invalid_flips = []\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    total_flips = 0\n",
    "\n",
    "    \n",
    "    for episode_id, true_emotions in val_labels_loaded.items():\n",
    "        if speaker in val_speaker_ids_loaded[episode_id]:\n",
    "            predicted_emotions = speaker_predictions_dict[speaker][episode_id]\n",
    "            #convert the emotions to the encoded values\n",
    "            true_emotions = [mapping[emotion] for emotion, spk in zip(true_emotions, val_speaker_ids_loaded[episode_id]) if spk == speaker]\n",
    "            valid, invalid = detect_emotion_flips(predicted_emotions, true_emotions)\n",
    "            total_flips += totalnum_flips(true_emotions)\n",
    "            valid_flips.extend(valid)\n",
    "            invalid_flips.extend(invalid)\n",
    "            true_labels.extend(true_emotions)\n",
    "            predicted_labels.extend(predicted_emotions)\n",
    "    \n",
    "    # Calculate accuracy of valid flips\n",
    "    correct_flips = len(valid_flips)\n",
    "    final+=total_flips\n",
    "    final_correct+=correct_flips\n",
    "    accuracy = correct_flips / total_flips if total_flips > 0 else 0\n",
    "    \n",
    "    # Calculate macro F1-score\n",
    "    macro_f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "    \n",
    "    speaker_metrics[speaker] = {'accuracy': accuracy, 'macro_f1': macro_f1}\n",
    "\n",
    "# print(\"Speaker-wise Metrics:\")\n",
    "# for speaker, metrics in speaker_metrics.items():\n",
    "#     print(f\"Speaker: {speaker}, Accuracy: {metrics['accuracy']}, Macro F1-score: {metrics['macro_f1']}\")\n",
    "\n",
    "# print(final_correct)\n",
    "# Calculate overall accuracy and macro F1-score\n",
    "overall_accuracy = final_correct / final\n",
    "overall_macro_f1 = np.mean([metrics['macro_f1'] for metrics in speaker_metrics.values()])\n",
    "\n",
    "print(f\"Overall Accuracy: {overall_accuracy}\")\n",
    "# print(f\"Overall Macro F1-score: {overall_macro_f1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
